
@article{alemiUncertaintyVariationalInformation2018,
  title = {Uncertainty in the {{Variational Information Bottleneck}}},
  author = {Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V.},
  date = {2018-07-02},
  url = {https://arxiv.org/abs/1807.00906v1},
  urldate = {2021-12-05},
  abstract = {We present a simple case study, demonstrating that Variational Information Bottleneck (VIB) can improve a network's classification calibration as well as its ability to detect out-of-distribution data. Without explicitly being designed to do so, VIB gives two natural metrics for handling and quantifying uncertainty.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/7RGDENEE/Alemi et al. - 2018 - Uncertainty in the Variational Information Bottlen.pdf;/Users/theofpa/Zotero/storage/N2HC39PV/1807.html}
}

@unpublished{azarbonyadWordsAreMalleable2017,
  title = {Words Are {{Malleable}}: {{Computing Semantic Shifts}} in {{Political}} and {{Media Discourse}}},
  shorttitle = {Words Are {{Malleable}}},
  author = {Azarbonyad, Hosein and Dehghani, Mostafa and Beelen, Kaspar and Arkut, Alexandra and Marx, Maarten and Kamps, Jaap},
  date = {2017-11-15},
  eprint = {1711.05603},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1711.05603},
  urldate = {2021-10-29},
  abstract = {Recently, researchers started to pay attention to the detection of temporal shifts in the meaning of words. However, most (if not all) of these approaches restricted their efforts to uncovering change over time, thus neglecting other valuable dimensions such as social or political variability. We propose an approach for detecting semantic shifts between different viewpoints—broadly defined as a set of texts that share a specific metadata feature, which can be a time-period, but also a social entity such as a political party. For each viewpoint, we learn a semantic space in which each word is represented as a low dimensional neural embedded vector. The challenge is to compare the meaning of a word in one space to its meaning in another space and measure the size of the semantic shifts. We compare the effectiveness of a measure based on optimal transformations between the two spaces with a measure based on the similarity of the neighbors of the word in the respective spaces. Our experiments demonstrate that the combination of these two performs best. We show that the semantic shifts not only occur over time, but also along different viewpoints in a short period of time. For evaluation, we demonstrate how this approach captures meaningful semantic shifts and can help improve other tasks such as the contrastive viewpoint summarization and ideology detection (measured as classification accuracy) in political texts. We also show that the two laws of semantic change which were empirically shown to hold for temporal shifts also hold for shifts across viewpoints. These laws state that frequent words are less likely to shift meaning while words with many senses are more likely to do so.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/theofpa/Zotero/storage/HZB3YFQT/Azarbonyad et al. - 2017 - Words are Malleable Computing Semantic Shifts in .pdf}
}

@article{baltrusaitisMultimodalMachineLearning2019,
  title = {Multimodal {{Machine Learning}}: {{A Survey}} and {{Taxonomy}}},
  shorttitle = {Multimodal {{Machine Learning}}},
  author = {Baltrušaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  date = {2019-02},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {41},
  number = {2},
  pages = {423--443},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2018.2798607},
  abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {Hidden Markov models,introductory,machine learning,Media,Multimedia communication,Multimodal,Speech,Speech recognition,Streaming media,survey,Visualization},
  file = {/Users/theofpa/Zotero/storage/24KF6ICM/Baltrušaitis et al. - 2019 - Multimodal Machine Learning A Survey and Taxonomy.pdf;/Users/theofpa/Zotero/storage/8AC25ERJ/8269806.html}
}

@inproceedings{baylorContinuousTrainingProduction2019,
  title = {Continuous {{Training}} for {{Production}} \{\vphantom\}{{ML}}\vphantom\{\} in the {{TensorFlow Extended}} (\{\vphantom\}{{TFX}}\vphantom\{\}) {{Platform}}},
  author = {Baylor, Denis and Haas, Kevin and Katsiapis, Konstantinos and Leong, Sammy and Liu, Rose and Menwald, Clemens and Miao, Hui and Polyzotis, Neoklis and Trott, Mitchell and Zinkevich, Martin},
  date = {2019},
  pages = {51--53},
  url = {https://www.usenix.org/conference/opml19/presentation/baylor},
  urldate = {2021-11-03},
  eventtitle = {2019 \{\vphantom\}{{USENIX}}\vphantom\{\} {{Conference}} on {{Operational Machine Learning}} ({{OpML}} 19)},
  isbn = {978-1-939133-00-7},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/ZHEQUKVE/Baylor et al. - 2019 - Continuous Training for Production ML in the Ten.pdf;/Users/theofpa/Zotero/storage/29J2LFUL/baylor.html}
}

@inproceedings{baylorContinuousTrainingProduction2019b,
  title = {Continuous {{Training}} for {{Production}} \{\vphantom\}{{ML}}\vphantom\{\} in the {{TensorFlow Extended}} (\{\vphantom\}{{TFX}}\vphantom\{\}) {{Platform}}},
  author = {Baylor, Denis and Haas, Kevin and Katsiapis, Konstantinos and Leong, Sammy and Liu, Rose and Menwald, Clemens and Miao, Hui and Polyzotis, Neoklis and Trott, Mitchell and Zinkevich, Martin},
  date = {2019},
  pages = {51--53},
  url = {https://www.usenix.org/conference/opml19/presentation/baylor},
  urldate = {2021-02-12},
  eventtitle = {2019 \{\vphantom\}{{USENIX}}\vphantom\{\} {{Conference}} on {{Operational Machine Learning}} ({{OpML}} 19)},
  isbn = {978-1-939133-00-7},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/A2MGNQYX/Continuous Training on TFX.pdf;/Users/theofpa/Zotero/storage/IFIPCGST/Baylor et al. - 2019 - Continuous Training for Production ML in the Ten.pdf;/Users/theofpa/Zotero/storage/EILJHK5N/baylor.html}
}

@inproceedings{benderClimbingNLUMeaning2020,
  title = {Climbing towards {{NLU}}: {{On Meaning}}, {{Form}}, and {{Understanding}} in the {{Age}} of {{Data}}},
  shorttitle = {Climbing towards {{NLU}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Bender, Emily M. and Koller, Alexander},
  date = {2020-07},
  pages = {5185--5198},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.463},
  url = {https://aclanthology.org/2020.acl-main.463},
  urldate = {2021-11-03},
  abstract = {The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We've Been and Where We're Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.},
  eventtitle = {{{ACL}} 2020},
  file = {/Users/theofpa/Zotero/storage/65JTB9XV/Bender and Koller - 2020 - Climbing towards NLU On Meaning, Form, and Unders.pdf}
}

@article{blandStatisticsNotesMultiple1995,
  title = {Statistics Notes: {{Multiple}} Significance Tests: The {{Bonferroni}} Method},
  shorttitle = {Statistics Notes},
  author = {Bland, J M. and Altman, D. G},
  date = {1995-01-21},
  journaltitle = {BMJ},
  shortjournal = {BMJ},
  volume = {310},
  number = {6973},
  pages = {170--170},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.310.6973.170},
  url = {https://www.bmj.com/lookup/doi/10.1136/bmj.310.6973.170},
  urldate = {2022-06-26},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/XGZMG56M/Bland and Altman - 1995 - Statistics notes Multiple significance tests the.pdf}
}

@article{bonifaziModelingDetectingAnomalous,
  title = {Modeling and {{Detecting Anomalous Safety Events}} in {{Approach Flights Using ADS-B Data}}},
  author = {Bonifazi, Alberto and Sun, Junzi and family=Baren, given=Gerben, prefix=van, useprefix=true and Hoekstra, Jacco},
  pages = {10},
  abstract = {Not all flight data anomalies correspond to operational safety concerns. But anomalous safety events can be linked to anomalies in flight data. During the final phases of a flight, two significant safety events are unstable approach and goaround. In this paper, using Automatic Dependent SurveillanceBroadcast (ADS-B) data, we develop several exceedance and anomaly detection techniques to identify these events. Rulebased algorithms and data-driven Gaussian Mixture Models (GMM) are proposed to identify unstable approaches. A fuzzy logic approach is developed to model and to identify go-arounds. We extend our analysis combining runway information and meteorological reports to provide deeper insights on flight safety during the approach. These identification models are also applied to the ADS-B data from the Schiphol Airport area in Amsterdam in 2018. By using a reference report provided by the Dutch transportation regulatory agency, the chosen GMM model can identify 25\% to 30\% of reported unstable approaches, and the go-around detection model can identify 98\% of go-arounds.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/TKL6ZZTR/Bonifazi et al. - Modeling and Detecting Anomalous Safety Events in .pdf}
}

@book{bouadiAdvancesIntelligentData2022,
  title = {Advances in {{Intelligent Data Analysis XX}}: 20th {{International Symposium}} on {{Intelligent Data Analysis}}, {{IDA}} 2022, {{Rennes}}, {{France}}, {{April}} 20–22, 2022, {{Proceedings}}},
  shorttitle = {Advances in {{Intelligent Data Analysis XX}}},
  editor = {Bouadi, Tassadit and Fromont, Elisa and Hüllermeier, Eyke},
  date = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {13205},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-031-01333-1},
  url = {https://link.springer.com/10.1007/978-3-031-01333-1},
  urldate = {2022-06-26},
  isbn = {978-3-031-01332-4 978-3-031-01333-1},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/GQES92ML/Bouadi et al. - 2022 - Advances in Intelligent Data Analysis XX 20th Int.pdf}
}

@article{breckDataValidationMachine,
  title = {Data {{Validation}} for {{Machine Learning}}},
  author = {Breck, Eric and Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong and Zinkevich, Martin},
  pages = {14},
  abstract = {Machine learning is a powerful tool for gleaning knowledge from massive amounts of data. While a great deal of machine learning research has focused on improving the accuracy and efficiency of training and inference algorithms, there is less attention in the equally important problem of monitoring the quality of data fed to machine learning. The importance of this problem is hard to dispute: errors in the input data can nullify any benefits on speed and accuracy for training and inference. This argument points to a data-centric approach to machine learning that treats training and serving data as an important production asset, on par with the algorithm and infrastructure used for learning.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/98EDTBUL/Breck et al. - Data Validation for Machine Learning.pdf}
}

@inproceedings{breunigLOFIdentifyingDensitybased2000,
  title = {{{LOF}}: Identifying Density-Based Local Outliers},
  shorttitle = {{{LOF}}},
  booktitle = {Proceedings of the 2000 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Breunig, Markus M. and Kriegel, Hans-Peter and Ng, Raymond T. and Sander, Jörg},
  date = {2000-05-16},
  series = {{{SIGMOD}} '00},
  pages = {93--104},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/342009.335388},
  url = {https://doi.org/10.1145/342009.335388},
  urldate = {2021-11-27},
  abstract = {For many KDD applications, such as detecting criminal activities in E-commerce, finding the rare instances or the outliers, can be more interesting than finding the common patterns. Existing work in outlier detection regards being an outlier as a binary property. In this paper, we contend that for many scenarios, it is more meaningful to assign to each object a degree of being an outlier. This degree is called the local outlier factor (LOF) of an object. It is local in that the degree depends on how isolated the object is with respect to the surrounding neighborhood. We give a detailed formal analysis showing that LOF enjoys many desirable properties. Using real-world datasets, we demonstrate that LOF can be used to find outliers which appear to be meaningful, but can otherwise not be identified with existing approaches. Finally, a careful performance evaluation of our algorithm confirms we show that our approach of finding local outliers can be practical.},
  isbn = {978-1-58113-217-5},
  keywords = {database mining,outlier detection},
  file = {/Users/theofpa/Zotero/storage/RGJWW98B/Breunig et al. - 2000 - LOF identifying density-based local outliers.pdf}
}

@software{buscheJarnoRFBIncense2022,
  title = {{{JarnoRFB}}/Incense},
  author = {Busche, Rüdiger},
  date = {2022-06-15T07:38:41Z},
  origdate = {2018-09-18T10:02:52Z},
  url = {https://github.com/JarnoRFB/incense},
  urldate = {2022-06-27},
  abstract = {Interactively retrieve data from sacred experiments.},
  keywords = {machine-learning,mongodb,sacred}
}

@inproceedings{cavenessTensorFlowDataValidation2020,
  title = {{{TensorFlow Data Validation}}: {{Data Analysis}} and {{Validation}} in {{Continuous ML Pipelines}}},
  shorttitle = {{{TensorFlow Data Validation}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Caveness, Emily and G. C., Paul Suganthan and Peng, Zhuo and Polyzotis, Neoklis and Roy, Sudip and Zinkevich, Martin},
  date = {2020-06-11},
  series = {{{SIGMOD}} '20},
  pages = {2793--2796},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3318464.3384707},
  url = {https://doi.org/10.1145/3318464.3384707},
  urldate = {2021-11-03},
  abstract = {Machine Learning (ML) research has primarily focused on improving the accuracy and efficiency of the training algorithms while paying much less attention to the equally important problem of understanding, validating, and monitoring the data fed to ML. Irrespective of the ML algorithms used, data errors can adversely affect the quality of the generated model. This indicates that we need to adopt a data-centric approach to ML that treats data as a first-class citizen, on par with algorithms and infrastructure which are the typical building blocks of ML pipelines. In this demonstration we showcase TensorFlow Data Validation (TFDV), a scalable data analysis and validation system for ML that we have developed at Google and recently open-sourced. This system is deployed in production as an integral part of TFX - an end-to-end machine learning platform at Google. It is used by hundreds of product teams at Google and has received significant attention from the open-source community as well.},
  isbn = {978-1-4503-6735-6},
  keywords = {data management,machine learning},
  file = {/Users/theofpa/Zotero/storage/7KPM5IES/Caveness et al. - 2020 - TensorFlow Data Validation Data Analysis and Vali.pdf}
}

@article{chandolaAnomalyDetectionSurvey2009,
  title = {Anomaly Detection: {{A}} Survey},
  shorttitle = {Anomaly Detection},
  author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
  date = {2009-07-30},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {41},
  number = {3},
  pages = {15:1--15:58},
  issn = {0360-0300},
  doi = {10.1145/1541880.1541882},
  url = {https://doi.org/10.1145/1541880.1541882},
  urldate = {2021-11-27},
  abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
  keywords = {Anomaly detection,outlier detection},
  file = {/Users/theofpa/Zotero/storage/DIZZSFLK/Chandola et al. - 2009 - Anomaly detection A survey.pdf}
}

@online{CitingTensorFlow,
  title = {Citing {{TensorFlow}}},
  url = {https://www.tensorflow.org/about/bib},
  urldate = {2022-06-27},
  langid = {english},
  organization = {{TensorFlow}},
  file = {/Users/theofpa/Zotero/storage/S9GU2L6R/bib.html}
}

@misc{cobbContextAwareDriftDetection2022,
  title = {Context-{{Aware Drift Detection}}},
  author = {Cobb, Oliver and Van Looveren, Arnaud},
  date = {2022-03-16},
  eprint = {2203.08644},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.08644},
  url = {http://arxiv.org/abs/2203.08644},
  urldate = {2022-06-26},
  abstract = {When monitoring machine learning systems, two-sample tests of homogeneity form the foundation upon which existing approaches to drift detection build. They are used to test for evidence that the distribution underlying recent deployment data differs from that underlying the historical reference data. Often, however, various factors such as time-induced correlation mean that batches of recent deployment data are not expected to form an i.i.d. sample from the historical data distribution. Instead we may wish to test for differences in the distributions conditional on \textbackslash textit\{context\} that is permitted to change. To facilitate this we borrow machinery from the causal inference domain to develop a more general drift detection framework built upon a foundation of two-sample tests for conditional distributional treatment effects. We recommend a particular instantiation of the framework based on maximum conditional mean discrepancies. We then provide an empirical study demonstrating its effectiveness for various drift detection problems of practical interest, such as detecting drift in the distributions underlying subpopulations of data in a manner that is insensitive to their respective prevalences. The study additionally demonstrates applicability to ImageNet-scale vision problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/theofpa/Zotero/storage/ZFH2SPAJ/Cobb and Van Looveren - 2022 - Context-Aware Drift Detection.pdf;/Users/theofpa/Zotero/storage/4WSUYPN9/2203.html}
}

@unpublished{cobbSequentialMultivariateChange2021,
  title = {Sequential {{Multivariate Change Detection}} with {{Calibrated}} and {{Memoryless False Detection Rates}}},
  author = {Cobb, Oliver and Van Looveren, Arnaud and Klaise, Janis},
  date = {2021-08-02},
  eprint = {2108.00883},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2108.00883},
  urldate = {2022-02-11},
  abstract = {Responding appropriately to the detections of a sequential change detector requires knowledge of the rate at which false positives occur in the absence of change. When the pre-change and post-change distributions are unknown, setting detection thresholds to achieve a desired false positive rate is challenging, even when there exists a large number of samples from the reference distribution. Existing works resort to setting time-invariant thresholds that focus on the expected runtime of the detector in the absence of change, either bounding it loosely from below or targeting it directly but with asymptotic arguments that we show cause significant miscalibration in practice. We present a simulation-based approach to setting time-varying thresholds that allows a desired expected runtime to be targeted with a 20x reduction in miscalibration whilst additionally keeping the false positive rate constant across time steps. Whilst the approach to threshold setting is metric agnostic, we show that when using the popular and powerful quadratic time MMD estimator, thoughtful structuring of the computation can reduce the cost during configuration from O(N2B) to O(N2 + NB) and during operation from O(N2) to O(N), where N is the number of reference samples and B the number of bootstrap samples. Code is made available as part of the open-source Python library alibi-detect.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/theofpa/Zotero/storage/SCKRT4N9/Cobb et al. - 2021 - Sequential Multivariate Change Detection with Cali.pdf}
}

@inproceedings{davidImpossibilityTheoremsDomain2010,
  title = {Impossibility {{Theorems}} for {{Domain Adaptation}}},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {David, Shai Ben and Lu, Tyler and Luu, Teresa and Pal, David},
  date = {2010-03-31},
  pages = {129--136},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v9/david10a.html},
  urldate = {2021-11-29},
  abstract = {The domain adaptation problem in machine learning occurs when the test data generating distribution differs from the one that generates the training data. It is clear that the success of learning under such circumstances depends on similarities between the two data distributions. We study assumptions about the relationship between the two distributions that one needed for domain adaptation learning to succeed.  We analyze the assumptions in an agnostic PAC-style learning model for a the setting in which the learner can access a labeled training data sample and an unlabeled sample generated by the test data distribution. We focus on three assumptions: (i) Similarity between the unlabeled distributions, (ii) Existence of a classifier in the hypothesis class with low error on both training and testing distributions, and (iii) The covariate shift assumption. I.e., the assumption that the conditioned label distribution (for each data point) is the same for both the training and test distributions.  We show that without either assumption (i) or (ii), the combination of the remaining assumptions is not sufficient toguarantee successful learning. Our negative results hold with respect to any domain adaptation learning algorithm, as long as it does not have access to target labeled examples.  In particular, we provide formal proofs that the popular covariate shift assumption is rather weak and does not relieve the necessity of the other assumptions.  We also discuss the intuitively appealing paradigm of reweighing the labeled training sample according to the target unlabeled distribution. We show that, somewhat counter intuitively, that paradigm cannot be trusted in the following sense. There are DA tasks that are indistinguishable, as far as the input training data goes, but in which reweighing leads to significant improvement in one task, while causing dramatic deterioration of the learning success in the other.},
  eventtitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/SFN83YIT/David et al. - 2010 - Impossibility Theorems for Domain Adaptation.pdf}
}

@article{deltrediciShortTermMeaningShift2018,
  title = {Short-{{Term Meaning Shift}}: {{A Distributional Exploration}}},
  shorttitle = {Short-{{Term Meaning Shift}}},
  author = {Del Tredici, Marco and Fernández, Raquel and Boleda, Gemma},
  date = {2018-09-10},
  url = {https://arxiv.org/abs/1809.03169v3},
  urldate = {2021-10-29},
  abstract = {We present the first exploration of meaning shift over short periods of time in online communities using distributional representations. We create a small annotated dataset and use it to assess the performance of a standard model for meaning shift detection on short-term meaning shift. We find that the model has problems distinguishing meaning shift from referential phenomena, and propose a measure of contextual variability to remedy this.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/8JTYSC72/Del Tredici et al. - 2018 - Short-Term Meaning Shift A Distributional Explora.pdf;/Users/theofpa/Zotero/storage/8IS3J2P3/1809.html}
}

@software{developersTensorFlow2022,
  title = {{{TensorFlow}}},
  author = {Developers, TensorFlow},
  date = {2022-05-23},
  doi = {10.5281/zenodo.6574269},
  url = {https://zenodo.org/record/6574269},
  urldate = {2022-06-27},
  abstract = {TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries, and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML-powered applications.},
  organization = {{Zenodo}},
  file = {/Users/theofpa/Zotero/storage/UU7HK7QY/6574269.html}
}

@misc{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019-05-24},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2022-06-24},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/theofpa/Zotero/storage/7MKIRW9K/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/Users/theofpa/Zotero/storage/MTV3TK3Y/1810.html}
}

@online{DistilbertbaseuncasedHuggingFace,
  title = {Distilbert-Base-Uncased · {{Hugging Face}}},
  url = {https://huggingface.co/distilbert-base-uncased},
  urldate = {2022-06-27},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {/Users/theofpa/Zotero/storage/CSZ6BYPF/distilbert-base-uncased.html}
}

@online{DistilrobertabaseHuggingFace,
  title = {Distilroberta-Base · {{Hugging Face}}},
  url = {https://huggingface.co/distilroberta-base},
  urldate = {2022-06-27},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {/Users/theofpa/Zotero/storage/KE9JXMNC/distilroberta-base.html}
}

@unpublished{eggOnlineLearningRecommendations2021,
  title = {Online {{Learning}} for {{Recommendations}} at {{Grubhub}}},
  author = {Egg, Alex},
  date = {2021-07-15},
  eprint = {2107.07106},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.1145/3460231.3474599},
  url = {http://arxiv.org/abs/2107.07106},
  urldate = {2021-09-28},
  abstract = {We propose a method to easily modify existing offline Recommender Systems to run online using Transfer Learning. Online Learning for Recommender Systems has two main advantages: quality and scale. Like many Machine Learning algorithms in production if not regularly retrained will suffer from Concept Drift. A policy that is updated frequently online can adapt to drift faster than a batch system. This is especially true for user-interaction systems like recommenders where the underlying distribution can shift drastically to follow user behaviour. As a platform grows rapidly like Grubhub, the cost of running batch training jobs becomes material. A shift from stateless batch learning offline to stateful incremental learning online can recover, for example, at Grubhub, up to a 45x cost savings and a +20\% metrics increase. There are a few challenges to overcome with the transition to online stateful learning, namely convergence, non-stationary embeddings and off-policy evaluation, which we explore from our experiences running this system in production.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/theofpa/Zotero/storage/P6CMUVYU/Egg - 2021 - Online Learning for Recommendations at Grubhub.pdf;/Users/theofpa/Zotero/storage/LLS7JCFE/2107.html}
}

@incollection{feldhansDriftDetectionText2021,
  title = {Drift {{Detection}} in {{Text Data}} with {{Document Embeddings}}},
  booktitle = {Intelligent {{Data Engineering}} and {{Automated Learning}} – {{IDEAL}} 2021},
  author = {Feldhans, Robert and Wilke, Adrian and Heindorf, Stefan and Shaker, Mohammad Hossein and Hammer, Barbara and Ngonga Ngomo, Axel-Cyrille and Hüllermeier, Eyke},
  editor = {Yin, Hujun and Camacho, David and Tino, Peter and Allmendinger, Richard and Tallón-Ballesteros, Antonio J. and Tang, Ke and Cho, Sung-Bae and Novais, Paulo and Nascimento, Susana},
  date = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {13113},
  pages = {107--118},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-91608-4_11},
  url = {https://link.springer.com/10.1007/978-3-030-91608-4_11},
  urldate = {2021-12-09},
  abstract = {Collections of text documents such as product reviews and microblogs often evolve over time. In practice, however, classifiers trained on them are updated infrequently, leading to performance degradation over time. While approaches for automatic drift detection have been proposed, they were often designed for low-dimensional sensor data, and it is unclear how well they perform for state-of-the-art text classifiers based on high-dimensional document embeddings. In this paper, we empirically compare drift detectors on document embeddings on two benchmarking datasets with varying amounts of drift. Our results show that multivariate drift detectors based on the Kernel Two-Sample Test and Least-Squares Density Difference outperform univariate drift detectors based on the Kolmogorov–Smirnov Test. Moreover, our experiments show that current drift detectors perform better on smaller embedding dimensions.},
  isbn = {978-3-030-91607-7 978-3-030-91608-4},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/GTMVPTRV/Feldhans et al. - 2021 - Drift Detection in Text Data with Document Embeddi.pdf}
}

@article{gamajoaoSurveyConceptDrift2014,
  title = {A Survey on Concept Drift Adaptation},
  author = {GamaJoão and ŽliobaitėIndrė and BifetAlbert and PechenizkiyMykola and BouchachiaAbdelhamid},
  date = {2014-03-01},
  journaltitle = {ACM Computing Surveys (CSUR)},
  publisher = {{ACM}},
  doi = {10.1145/2523813},
  url = {https://dl.acm.org/doi/abs/10.1145/2523813},
  urldate = {2022-06-26},
  abstract = {Concept drift primarily refers to an online supervised learning scenario when the relation between the input data and the target variable changes over time. Assuming a general knowledge of supervised learning in this article, we characterize adaptive ...},
  langid = {english},
  annotation = {PUB27 		New York, NY, USA},
  file = {/Users/theofpa/Zotero/storage/HLG7DXG7/GamaJoão et al. - 2014 - A survey on concept drift adaptation.pdf;/Users/theofpa/Zotero/storage/D96RLP5G/2523813.html}
}

@inproceedings{gamaLearningLocalDrift2006,
  title = {Learning with {{Local Drift Detection}}},
  booktitle = {Advanced {{Data Mining}} and {{Applications}}},
  author = {Gama, João and Castillo, Gladys},
  editor = {Li, Xue and Zaïane, Osmar R. and Li, Zhanhuai},
  date = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {42--55},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/11811305_4},
  abstract = {Most of the work in Machine Learning assume that examples are generated at random according to some stationary probability distribution. In this work we study the problem of learning when the distribution that generates the examples changes over time. We present a method for detection of changes in the probability distribution of examples. The idea behind the drift detection method is to monitor the online error-rate of a learning algorithm looking for significant deviations. The method can be used as a wrapper over any learning algorithm. In most problems, a change affects only some regions of the instance space, not the instance space as a whole. In decision models that fit different functions to regions of the instance space, like Decision Trees and Rule Learners, the method can be used to monitor the error in regions of the instance space, with advantages of fast model adaptation. In this work we present experiments using the method as a wrapper over a decision tree and a linear model, and in each internal-node of a decision tree. The experimental results obtained in controlled experiments using artificial data and a real-world problem show a good performance detecting drift and in adapting the decision model to the new concept.},
  isbn = {978-3-540-37026-0},
  langid = {english},
  keywords = {Concept Change,Data Stream,Decision Model,Decision Tree,Learning Algorithm},
  file = {/Users/theofpa/Zotero/storage/V4DHYTN8/Gama and Castillo - 2006 - Learning with Local Drift Detection.pdf}
}

@article{goelRobustnessGymUnifying2021,
  title = {Robustness {{Gym}}: {{Unifying}} the {{NLP Evaluation Landscape}}},
  shorttitle = {Robustness {{Gym}}},
  author = {Goel, Karan and Rajani, Nazneen and Vig, Jesse and Tan, Samson and Wu, Jason and Zheng, Stephan and Xiong, Caiming and Bansal, Mohit and Ré, Christopher},
  date = {2021-01-13},
  url = {https://arxiv.org/abs/2101.04840v1},
  urldate = {2021-11-03},
  abstract = {Despite impressive performance on standard benchmarks, deep neural networks are often brittle when deployed in real-world systems. Consequently, recent research has focused on testing the robustness of such models, resulting in a diverse set of evaluation methodologies ranging from adversarial attacks to rule-based data transformations. In this work, we identify challenges with evaluating NLP systems and propose a solution in the form of Robustness Gym (RG), a simple and extensible evaluation toolkit that unifies 4 standard evaluation paradigms: subpopulations, transformations, evaluation sets, and adversarial attacks. By providing a common platform for evaluation, Robustness Gym enables practitioners to compare results from all 4 evaluation paradigms with just a few clicks, and to easily develop and share novel evaluation methods using a built-in set of abstractions. To validate Robustness Gym's utility to practitioners, we conducted a real-world case study with a sentiment-modeling team, revealing performance degradations of 18\%+. To verify that Robustness Gym can aid novel research analyses, we perform the first study of state-of-the-art commercial and academic named entity linking (NEL) systems, as well as a fine-grained analysis of state-of-the-art summarization models. For NEL, commercial systems struggle to link rare entities and lag their academic counterparts by 10\%+, while state-of-the-art summarization models struggle on examples that require abstraction and distillation, degrading by 9\%+. Robustness Gym can be found at https://robustnessgym.com/},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/JZXACBJZ/Goel et al. - 2021 - Robustness Gym Unifying the NLP Evaluation Landsc.pdf;/Users/theofpa/Zotero/storage/Y27AITIR/2101.html}
}

@article{goodfellowGenerativeAdversarialNetworks2020,
  title = {Generative Adversarial Networks},
  author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2020-10-22},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {63},
  number = {11},
  pages = {139--144},
  issn = {0001-0782},
  doi = {10.1145/3422622},
  url = {https://doi.org/10.1145/3422622},
  urldate = {2022-01-31},
  abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
  file = {/Users/theofpa/Zotero/storage/EIT8V4JX/Goodfellow et al. - 2020 - Generative adversarial networks.pdf}
}

@article{grettonKernelTwoSampleTest2012,
  title = {A {{Kernel Two-Sample Test}}},
  author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Schölkopf, Bernhard and Smola, Alexander},
  date = {2012},
  journaltitle = {Journal of Machine Learning Research},
  volume = {13},
  number = {25},
  pages = {723--773},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v13/gretton12a.html},
  urldate = {2021-12-07},
  abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD). We present two distribution-free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
  file = {/Users/theofpa/Zotero/storage/9NZRR6LD/Gretton et al. - 2012 - A Kernel Two-Sample Test.pdf}
}

@article{hamiltonDiachronicWordEmbeddings2016,
  title = {Diachronic {{Word Embeddings Reveal Statistical Laws}} of {{Semantic Change}}},
  author = {Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
  date = {2016-05-30},
  url = {https://arxiv.org/abs/1605.09096v6},
  urldate = {2021-11-03},
  abstract = {Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity---the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation---independent of frequency, words that are more polysemous have higher rates of semantic change.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/IS9MAAPN/Hamilton et al. - 2016 - Diachronic Word Embeddings Reveal Statistical Laws.pdf;/Users/theofpa/Zotero/storage/XD9YKR3M/1605.html}
}

@article{hendrycksBaselineDetectingMisclassified2016,
  title = {A {{Baseline}} for {{Detecting Misclassified}} and {{Out-of-Distribution Examples}} in {{Neural Networks}}},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  date = {2016-10-07},
  url = {https://arxiv.org/abs/1610.02136v3},
  urldate = {2021-12-05},
  abstract = {We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/FQMUSEXG/Hendrycks and Gimpel - 2016 - A Baseline for Detecting Misclassified and Out-of-.pdf;/Users/theofpa/Zotero/storage/6NFK8JKN/1610.html}
}

@article{hendrycksDeepAnomalyDetection2018,
  title = {Deep {{Anomaly Detection}} with {{Outlier Exposure}}},
  author = {Hendrycks, Dan and Mazeika, Mantas and Dietterich, Thomas},
  date = {2018-12-11},
  url = {https://arxiv.org/abs/1812.04606v3},
  urldate = {2021-12-05},
  abstract = {It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/2TFT7YC8/Hendrycks et al. - 2018 - Deep Anomaly Detection with Outlier Exposure.pdf;/Users/theofpa/Zotero/storage/QEPETSPU/1812.html}
}

@article{hodgesSignificanceProbabilitySmirnov1958,
  title = {The Significance Probability of the Smirnov Two-Sample Test},
  author = {Hodges, J. L.},
  date = {1958-01-01},
  journaltitle = {Arkiv för Matematik},
  shortjournal = {Ark. Mat.},
  volume = {3},
  number = {5},
  pages = {469--486},
  issn = {1871-2487},
  doi = {10.1007/BF02589501},
  url = {https://doi.org/10.1007/BF02589501},
  urldate = {2022-06-26},
  langid = {english},
  keywords = {Continuity Correction,Empirical Distribution,Heuristic Argument,Kolmogorov Test,Notational Complication},
  file = {/Users/theofpa/Zotero/storage/2Y9SFZ5V/Hodges - 1958 - The significance probability of the smirnov two-sa.pdf}
}

@unpublished{huangImportanceGradientsDetecting2021,
  title = {On the {{Importance}} of {{Gradients}} for {{Detecting Distributional Shifts}} in the {{Wild}}},
  author = {Huang, Rui and Geng, Andrew and Li, Yixuan},
  date = {2021-10-09},
  eprint = {2110.00218},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2110.00218},
  urldate = {2021-12-06},
  abstract = {Detecting out-of-distribution (OOD) data has become a critical component in ensuring the safe deployment of machine learning models in the real world. Existing OOD detection approaches primarily rely on the output or feature space for deriving OOD scores, while largely overlooking information from the gradient space. In this paper, we present GradNorm, a simple and effective approach for detecting OOD inputs by utilizing information extracted from the gradient space. GradNorm directly employs the vector norm of gradients, backpropagated from the KL divergence between the softmax output and a uniform probability distribution. Our key idea is that the magnitude of gradients is higher for indistribution (ID) data than that for OOD data, making it informative for OOD detection. GradNorm demonstrates superior performance, reducing the average FPR95 by up to 16.33\% compared to the previous best method. Code and data available: https://github.com/deeplearning-wisc/gradnorm\_ood.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/theofpa/Zotero/storage/QS8TLFD7/Huang et al. - 2021 - On the Importance of Gradients for Detecting Distr.pdf}
}

@article{JMLR:v12:pedregosa11a,
  title = {Scikit-Learn: {{Machine}} Learning in Python},
  author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  number = {85},
  pages = {2825--2830},
  url = {http://jmlr.org/papers/v12/pedregosa11a.html}
}

@article{kimTemporalAnalysisLanguage2014,
  title = {Temporal {{Analysis}} of {{Language}} through {{Neural Language Models}}},
  author = {Kim, Yoon and Chiu, Yi-I. and Hanaki, Kentaro and Hegde, Darshan and Petrov, Slav},
  date = {2014-05-14},
  url = {https://arxiv.org/abs/1405.3515v1},
  urldate = {2021-11-03},
  abstract = {We provide a method for automatically detecting change in language across time through a chronologically trained neural language model. We train the model on the Google Books Ngram corpus to obtain word vector representations specific to each year, and identify words that have changed significantly from 1900 to 2009. The model identifies words such as "cell" and "gay" as having changed during that time period. The model simultaneously identifies the specific years during which such words underwent change.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/696CXERW/Kim et al. - 2014 - Temporal Analysis of Language through Neural Langu.pdf;/Users/theofpa/Zotero/storage/X6NJQDIM/1405.html}
}

@unpublished{klaiseMonitoringExplainabilityModels2020,
  title = {Monitoring and Explainability of Models in Production},
  author = {Klaise, Janis and Van Looveren, Arnaud and Cox, Clive and Vacanti, Giovanni and Coca, Alexandru},
  date = {2020-07-13},
  eprint = {2007.06299},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2007.06299},
  urldate = {2021-12-09},
  abstract = {The machine learning lifecycle extends beyond the deployment stage. Monitoring deployed models is crucial for continued provision of high quality machine learning enabled services. Key areas include model performance and data monitoring, detecting outliers and data drift using statistical techniques, and providing explanations of historic predictions. We discuss the challenges to successful implementation of solutions in each of these areas with some recent examples of production ready solutions using open source tools.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/theofpa/Zotero/storage/8JAWFPDQ/Klaise et al. - 2020 - Monitoring and explainability of models in product.pdf}
}

@unpublished{klaiseMonitoringExplainabilityModels2020a,
  title = {Monitoring and Explainability of Models in Production},
  author = {Klaise, Janis and Van Looveren, Arnaud and Cox, Clive and Vacanti, Giovanni and Coca, Alexandru},
  date = {2020-07-13},
  eprint = {2007.06299},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2007.06299},
  urldate = {2021-12-08},
  abstract = {The machine learning lifecycle extends beyond the deployment stage. Monitoring deployed models is crucial for continued provision of high quality machine learning enabled services. Key areas include model performance and data monitoring, detecting outliers and data drift using statistical techniques, and providing explanations of historic predictions. We discuss the challenges to successful implementation of solutions in each of these areas with some recent examples of production ready solutions using open source tools.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/theofpa/Zotero/storage/TN6KWBMW/Klaise et al. - 2020 - Monitoring and explainability of models in product.pdf}
}

@inproceedings{klaus_greff-proc-scipy-2017,
  title = {The {{Sacred Infrastructure}} for {{Computational Research}}},
  booktitle = {Proceedings of the 16th {{Python}} in {{Science Conference}}},
  author = {Greff, Klaus and Klein, Aaron and Chovanec, Martin and Hutter, Frank and {Jürgen Schmidhuber}},
  editor = {Huff, Katy and Lippa, David and Niederhut, Dillon and {M Pacer}},
  date = {2017},
  pages = {49--56},
  doi = {10.25080/shinma-7f4c6e7-008}
}

@article{kohWILDSBenchmarkIntheWild2020,
  title = {{{WILDS}}: {{A Benchmark}} of in-the-{{Wild Distribution Shifts}}},
  shorttitle = {{{WILDS}}},
  author = {Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and Lee, Tony and David, Etienne and Stavness, Ian and Guo, Wei and Earnshaw, Berton A. and Haque, Imran S. and Beery, Sara and Leskovec, Jure and Kundaje, Anshul and Pierson, Emma and Levine, Sergey and Finn, Chelsea and Liang, Percy},
  date = {2020-12-14},
  url = {https://arxiv.org/abs/2012.07421v3},
  urldate = {2021-12-09},
  abstract = {Distribution shifts -- where the training distribution differs from the test distribution -- can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. Code and leaderboards are available at https://wilds.stanford.edu.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/R629ANC2/Koh et al. - 2020 - WILDS A Benchmark of in-the-Wild Distribution Shi.pdf;/Users/theofpa/Zotero/storage/3VV8QKXI/2012.html}
}

@unpublished{konstantinosMLEngineeringBrief2020,
  title = {Towards {{ML Engineering}}: {{A Brief History Of TensorFlow Extended}} ({{TFX}})},
  shorttitle = {Towards {{ML Engineering}}},
  author = {Konstantinos and Katsiapis and Karmarkar, Abhijit and Altay, Ahmet and Zaks, Aleksandr and Polyzotis, Neoklis and Ramesh, Anusha and Mathes, Ben and Vasudevan, Gautam and Giannoumis, Irene and Wilkiewicz, Jarek and Simsa, Jiri and Hong, Justin and Trott, Mitch and Lutz, Noé and Dournov, Pavel A. and Crowe, Robert and Sirajuddin, Sarah and Warkentin, Tris Brian and Li, Zhitao},
  date = {2020-10-07},
  eprint = {2010.02013},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2010.02013},
  urldate = {2021-02-15},
  abstract = {Software Engineering, as a discipline, has matured over the past 5+ decades. The modern world heavily depends on it, so the increased maturity of Software Engineering was an eventuality. Practices like testing and reliable technologies help make Software Engineering reliable enough to build industries upon. Meanwhile, Machine Learning (ML) has also grown over the past 2+ decades. ML is used more and more for research, experimentation and production workloads. ML now commonly powers widely-used products integral to our lives. But ML Engineering, as a discipline, has not widely matured as much as its Software Engineering ancestor. Can we take what we have learned and help the nascent field of applied ML evolve into ML Engineering the way Programming evolved into Software Engineering [1]? In this article we will give a whirlwind tour of Sibyl [2] and TensorFlow Extended (TFX) [3], two successive end-to-end (E2E) ML platforms at Alphabet. We will share the lessons learned from over a decade of applied ML built on these platforms, explain both their similarities and their differences, and expand on the shifts (both mental and technical) that helped us on our journey. In addition, we will highlight some of the capabilities of TFX that help realize several aspects of ML Engineering. We argue that in order to unlock the gains ML can bring, organizations should advance the maturity of their ML teams by investing in robust ML infrastructure and promoting ML Engineering education. We also recommend that before focusing on cutting-edge ML modeling techniques, product leaders should invest more time in adopting interoperable ML platforms for their organizations. In closing, we will also share a glimpse into the future of TFX.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/theofpa/Zotero/storage/3748E5AS/Konstantinos et al. - 2020 - Towards ML Engineering A Brief History Of TensorF.pdf;/Users/theofpa/Zotero/storage/G7WJB2XC/2010.html}
}

@misc{kublerAutoMLTwoSampleTest2022,
  title = {{{AutoML Two-Sample Test}}},
  author = {Kübler, Jonas M. and Stimper, Vincent and Buchholz, Simon and Muandet, Krikamol and Schölkopf, Bernhard},
  date = {2022-06-17},
  eprint = {2206.08843},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.08843},
  urldate = {2022-06-26},
  abstract = {Two-sample tests are important in statistics and machine learning, both as tools for scientific discovery as well as to detect distribution shifts. This led to the development of many sophisticated test procedures going beyond the standard supervised learning frameworks, whose usage can require specialized knowledge about two-sample testing. We use a simple test that takes the mean discrepancy of a witness function as the test statistic and prove that minimizing a squared loss leads to a witness with optimal testing power. This allows us to leverage recent advancements in AutoML. Without any user input about the problems at hand, and using the same method for all our experiments, our AutoML two-sample test achieves competitive performance on a diverse distribution shift benchmark as well as on challenging two-sample testing problems. We provide an implementation of the AutoML two-sample test in the Python package autotst.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/theofpa/Zotero/storage/2ANZSNEQ/Kübler et al. - 2022 - AutoML Two-Sample Test.pdf;/Users/theofpa/Zotero/storage/2R9MDI68/2206.html}
}

@inproceedings{kulkarniStatisticallySignificantDetection2015,
  title = {Statistically {{Significant Detection}} of {{Linguistic Change}}},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{World Wide Web}}},
  author = {Kulkarni, Vivek and Al-Rfou, Rami and Perozzi, Bryan and Skiena, Steven},
  date = {2015-05-18},
  series = {{{WWW}} '15},
  pages = {625--635},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  location = {{Republic and Canton of Geneva, CHE}},
  doi = {10.1145/2736277.2741627},
  url = {https://doi.org/10.1145/2736277.2741627},
  urldate = {2021-11-03},
  abstract = {We propose a new computational approach for tracking and detecting statistically significant linguistic shifts in the meaning and usage of words. Such linguistic shifts are especially prevalent on the Internet, where the rapid exchange of ideas can quickly change a word's meaning. Our meta-analysis approach constructs property time series of word usage, and then uses statistically sound change point detection algorithms to identify significant linguistic shifts. We consider and analyze three approaches of increasing complexity to generate such linguistic property time series, the culmination of which uses distributional characteristics inferred from word co-occurrences. Using recently proposed deep neural language models, we first train vector representations of words for each time period. Second, we warp the vector spaces into one unified coordinate system. Finally, we construct a distance-based distributional time series for each word to track its linguistic displacement over time. We demonstrate that our approach is scalable by tracking linguistic change across years of micro-blogging using Twitter, a decade of product reviews using a corpus of movie reviews from Amazon, and a century of written books using the Google Book Ngrams. Our analysis reveals interesting patterns of language usage change commensurate with each medium.},
  isbn = {978-1-4503-3469-3},
  keywords = {computational linguistics,web mining},
  file = {/Users/theofpa/Zotero/storage/492IHCD9/Kulkarni et al. - 2015 - Statistically Significant Detection of Linguistic .pdf}
}

@article{laiSequentialChangepointDetection1995,
  title = {Sequential {{Changepoint Detection}} in {{Quality Control}} and {{Dynamical Systems}}},
  author = {Lai, Tze Leung},
  date = {1995},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {57},
  number = {4},
  pages = {613--644},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1995.tb02052.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1995.tb02052.x},
  urldate = {2022-02-11},
  abstract = {After a brief survey of a large variety of sequential detection procedures that are widely scattered in statistical references on quality control and engineering references on fault detection and signal processing, we study some open problems concerning these procedures and introduce a unified theory of sequential changepoint detection. This theory leads to a class of sequential detection rules which are not too demanding in computational and memory requirements for on-line implementation and yet are nearly optimal under several performance criteria.},
  langid = {english},
  keywords = {detection delay,false alarm,fault detection,generalized likelihood ratio statistics,moving averages,quality control},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1995.tb02052.x},
  file = {/Users/theofpa/Zotero/storage/B7YUNFP4/Lai - 1995 - Sequential Changepoint Detection in Quality Contro.pdf;/Users/theofpa/Zotero/storage/42VJHWE2/j.2517-6161.1995.tb02052.html}
}

@article{liptonDetectingCorrectingLabel2018,
  title = {Detecting and {{Correcting}} for {{Label Shift}} with {{Black Box Predictors}}},
  author = {Lipton, Zachary C. and Wang, Yu-Xiang and Smola, Alex},
  date = {2018-02-12},
  url = {https://arxiv.org/abs/1802.03916v3},
  urldate = {2021-11-29},
  abstract = {Faced with distribution shift between training and test set, we wish to detect and quantify the shift, and to correct our classifiers without test set labels. Motivated by medical diagnosis, where diseases (targets) cause symptoms (observations), we focus on label shift, where the label marginal \$p(y)\$ changes but the conditional \$p(x| y)\$ does not. We propose Black Box Shift Estimation (BBSE) to estimate the test distribution \$p(y)\$. BBSE exploits arbitrary black box predictors to reduce dimensionality prior to shift correction. While better predictors give tighter estimates, BBSE works even when predictors are biased, inaccurate, or uncalibrated, so long as their confusion matrices are invertible. We prove BBSE's consistency, bound its error, and introduce a statistical test that uses BBSE to detect shift. We also leverage BBSE to correct classifiers. Experiments demonstrate accurate estimates and improved prediction, even on high-dimensional datasets of natural images.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/63L3W4LF/Lipton et al. - 2018 - Detecting and Correcting for Label Shift with Blac.pdf;/Users/theofpa/Zotero/storage/3TMJ8HGQ/1802.html}
}

@inproceedings{liuIsolationForest2008,
  title = {Isolation {{Forest}}},
  booktitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}}},
  author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
  date = {2008-12},
  pages = {413--422},
  issn = {2374-8486},
  doi = {10.1109/ICDM.2008.17},
  abstract = {Most existing model-based approaches to anomaly detection construct a profile of normal instances, then identify instances that do not conform to the normal profile as anomalies. This paper proposes a fundamentally different model-based method that explicitly isolates anomalies instead of profiles normal points. To our best knowledge, the concept of isolation has not been explored in current literature. The use of isolation enables the proposed method, iForest, to exploit sub-sampling to an extent that is not feasible in existing methods, creating an algorithm which has a linear time complexity with a low constant and a low memory requirement. Our empirical evaluation shows that iForest performs favourably to ORCA, a near-linear time complexity distance-based method, LOF and random forests in terms of AUC and processing time, and especially in large data sets. iForest also works well in high dimensional problems which have a large number of irrelevant attributes, and in situations where training set does not contain any anomalies.},
  eventtitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}}},
  keywords = {anomaly detection,Application software,Astronomy,binary trees,Constraint optimization,Credit cards,Data mining,Detectors,Information technology,isolation forest,Isolation technology,Laboratories,model based,novelty detection,outlier detection,Performance evaluation},
  file = {/Users/theofpa/Zotero/storage/7VST2N2X/Liu et al. - 2008 - Isolation Forest.pdf;/Users/theofpa/Zotero/storage/U3RPNE6G/4781136.html}
}

@article{liuLearningDeepKernels2020,
  title = {Learning {{Deep Kernels}} for {{Non-Parametric Two-Sample Tests}}},
  author = {Liu, Feng and Xu, Wenkai and Lu, Jie and Zhang, Guangquan and Gretton, Arthur and Sutherland, Danica J.},
  date = {2020-02-21},
  url = {https://arxiv.org/abs/2002.09116v3},
  urldate = {2022-02-02},
  abstract = {We propose a class of kernel-based two-sample tests, which aim to determine whether two sets of samples are drawn from the same distribution. Our tests are constructed from kernels parameterized by deep neural nets, trained to maximize test power. These tests adapt to variations in distribution smoothness and shape over space, and are especially suited to high dimensions and complex data. By contrast, the simpler kernels used in prior kernel testing work are spatially homogeneous, and adaptive only in lengthscale. We explain how this scheme includes popular classifier-based two-sample tests as a special case, but improves on them in general. We provide the first proof of consistency for the proposed adaptation method, which applies both to kernels on deep features and to simpler radial basis kernels or multiple kernel learning. In experiments, we establish the superior performance of our deep kernels in hypothesis testing on benchmark and real-world data. The code of our deep-kernel-based two sample tests is available at https://github.com/fengliu90/DK-for-TST.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/8BMYBY2K/Liu et al. - 2020 - Learning Deep Kernels for Non-Parametric Two-Sampl.pdf;/Users/theofpa/Zotero/storage/SJ6DTC6D/2002.html}
}

@misc{lopez-pazRevisitingClassifierTwoSample2018,
  title = {Revisiting {{Classifier Two-Sample Tests}}},
  author = {Lopez-Paz, David and Oquab, Maxime},
  date = {2018-03-13},
  eprint = {1610.06545},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1610.06545},
  urldate = {2022-06-26},
  abstract = {The goal of two-sample tests is to assess whether two samples, \$S\_P \textbackslash sim P\^n\$ and \$S\_Q \textbackslash sim Q\^m\$, are drawn from the same distribution. Perhaps intriguingly, one relatively unexplored method to build two-sample tests is the use of binary classifiers. In particular, construct a dataset by pairing the \$n\$ examples in \$S\_P\$ with a positive label, and by pairing the \$m\$ examples in \$S\_Q\$ with a negative label. If the null hypothesis "\$P = Q\$" is true, then the classification accuracy of a binary classifier on a held-out subset of this dataset should remain near chance-level. As we will show, such Classifier Two-Sample Tests (C2ST) learn a suitable representation of the data on the fly, return test statistics in interpretable units, have a simple null distribution, and their predictive uncertainty allow to interpret where \$P\$ and \$Q\$ differ. The goal of this paper is to establish the properties, performance, and uses of C2ST. First, we analyze their main theoretical properties. Second, we compare their performance against a variety of state-of-the-art alternatives. Third, we propose their use to evaluate the sample quality of generative models with intractable likelihoods, such as Generative Adversarial Networks (GANs). Fourth, we showcase the novel application of GANs together with C2ST for causal discovery.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/theofpa/Zotero/storage/LKBURGRZ/Lopez-Paz and Oquab - 2018 - Revisiting Classifier Two-Sample Tests.pdf;/Users/theofpa/Zotero/storage/KM2ASBIN/1610.html}
}

@article{ltdAlibidetectDocumentationb,
  title = {Alibi-Detect {{Documentation}}},
  author = {Ltd, Seldon Technologies},
  url = {https://docs.seldon.io/projects/alibi-detect/en/v0.9.1/},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/SINMEI5F/Ltd - alibi-detect Documentation.pdf}
}

@article{luLearningConceptDrift2018,
  title = {Learning under {{Concept Drift}}: {{A Review}}},
  shorttitle = {Learning under {{Concept Drift}}},
  author = {Lu, Jie and Liu, Anjin and Dong, Fan and Gu, Feng and Gama, Joao and Zhang, Guangquan},
  date = {2018},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  eprint = {2004.05785},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {1--1},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2018.2876857},
  url = {http://arxiv.org/abs/2004.05785},
  urldate = {2022-06-26},
  abstract = {Concept drift describes unforeseeable changes in the underlying distribution of streaming data over time. Concept drift research involves the development of methodologies and techniques for drift detection, understanding and adaptation. Data analysis has revealed that machine learning in a concept drift environment will result in poor learning results if the drift is not addressed. To help researchers identify which research topics are significant and how to apply related techniques in data analysis tasks, it is necessary that a high quality, instructive review of current research developments and trends in the concept drift field is conducted. In addition, due to the rapid development of concept drift in recent years, the methodologies of learning under concept drift have become noticeably systematic, unveiling a framework which has not been mentioned in literature. This paper reviews over 130 high quality publications in concept drift related research areas, analyzes up-to-date developments in methodologies and techniques, and establishes a framework of learning under concept drift including three main components: concept drift detection, concept drift understanding, and concept drift adaptation. This paper lists and discusses 10 popular synthetic datasets and 14 publicly available benchmark datasets used for evaluating the performance of learning algorithms aiming at handling concept drift. Also, concept drift related research directions are covered and discussed. By providing state-of-the-art knowledge, this survey will directly support researchers in their understanding of research developments in the field of learning under concept drift.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/theofpa/Zotero/storage/ZZVDKHHT/Lu et al. - 2018 - Learning under Concept Drift A Review.pdf;/Users/theofpa/Zotero/storage/C3BRYUD2/2004.html}
}

@online{MachineLearningDesign,
  title = {Machine {{Learning Design Patterns}} [{{Book}}]},
  url = {https://www.oreilly.com/library/view/machine-learning-design/9781098115777/},
  urldate = {2022-06-25},
  abstract = {The design patterns in this book capture best practices and solutions to recurring problems in machine learning. The authors, three Google engineers, catalog proven methods to help data scientists tackle … - Selection from Machine Learning Design Patterns [Book]},
  isbn = {9781098115784},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/2FK98H7D/9781098115777.html}
}

@inproceedings{mikolovDistributedRepresentationsWords2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  date = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
  urldate = {2021-10-29},
  file = {/Users/theofpa/Zotero/storage/K7F35TT3/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf}
}

@article{millerEffectNaturalDistribution2020,
  title = {The {{Effect}} of {{Natural Distribution Shift}} on {{Question Answering Models}}},
  author = {Miller, John and Krauth, Karl and Recht, Benjamin and Schmidt, Ludwig},
  date = {2020-04-29},
  url = {https://arxiv.org/abs/2004.14444v1},
  urldate = {2021-11-03},
  abstract = {We build four new test sets for the Stanford Question Answering Dataset (SQuAD) and evaluate the ability of question-answering systems to generalize to new data. Our first test set is from the original Wikipedia domain and measures the extent to which existing systems overfit the original test set. Despite several years of heavy test set re-use, we find no evidence of adaptive overfitting. The remaining three test sets are constructed from New York Times articles, Reddit posts, and Amazon product reviews and measure robustness to natural distribution shifts. Across a broad range of models, we observe average performance drops of 3.8, 14.0, and 17.4 F1 points, respectively. In contrast, a strong human baseline matches or exceeds the performance of SQuAD models on the original domain and exhibits little to no drop in new domains. Taken together, our results confirm the surprising resilience of the holdout method and emphasize the need to move towards evaluation metrics that incorporate robustness to natural distribution shifts.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/QZAK3NWV/Miller et al. - 2020 - The Effect of Natural Distribution Shift on Questi.pdf;/Users/theofpa/Zotero/storage/2SXGYHNZ/2004.html}
}

@online{ModelDBProceedingsWorkshop,
  title = {{{ModelDB}} | {{Proceedings}} of the {{Workshop}} on {{Human-In-the-Loop Data Analytics}}},
  url = {https://dl.acm.org/doi/10.1145/2939502.2939516},
  urldate = {2021-11-03},
  file = {/Users/theofpa/Zotero/storage/MSEXWQ2A/Vartak et al. - 2016 - M odelspa.pdf;/Users/theofpa/Zotero/storage/YEL2PX3L/2939502.html}
}

@article{morrisTextAttackFrameworkAdversarial2020,
  title = {{{TextAttack}}: {{A Framework}} for {{Adversarial Attacks}}, {{Data Augmentation}}, and {{Adversarial Training}} in {{NLP}}},
  shorttitle = {{{TextAttack}}},
  author = {Morris, John X. and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
  date = {2020-04-29},
  url = {https://arxiv.org/abs/2005.05909v4},
  urldate = {2021-11-03},
  abstract = {While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack's modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness. TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/BBVH3T9W/Morris et al. - 2020 - TextAttack A Framework for Adversarial Attacks, D.pdf;/Users/theofpa/Zotero/storage/VEGKLKTB/2005.html}
}

@inproceedings{niJustifyingRecommendationsUsing2019,
  title = {Justifying {{Recommendations}} Using {{Distantly-Labeled Reviews}} and {{Fine-Grained Aspects}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Ni, Jianmo and Li, Jiacheng and McAuley, Julian},
  date = {2019},
  pages = {188--197},
  publisher = {{Association for Computational Linguistics}},
  location = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1018},
  url = {https://www.aclweb.org/anthology/D19-1018},
  urldate = {2022-06-24},
  eventtitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/JFCA5NNP/Ni et al. - 2019 - Justifying Recommendations using Distantly-Labeled.pdf}
}

@inproceedings{nishidaDetectingConceptDrift2007,
  title = {Detecting {{Concept Drift Using Statistical Testing}}},
  booktitle = {Discovery {{Science}}},
  author = {Nishida, Kyosuke and Yamauchi, Koichiro},
  editor = {Corruble, Vincent and Takeda, Masayuki and Suzuki, Einoshin},
  date = {2007},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {264--269},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75488-6_27},
  abstract = {Detecting concept drift is important for dealing with real-world online learning problems. To detect concept drift in a small number of examples, methods that have an online classifier and monitor its prediction errors during the learning have been developed. We have developed such a detection method that uses a statistical test of equal proportions. Experimental results showed that our method performed well in detecting the concept drift in five synthetic datasets that contained various types of concept drift.},
  isbn = {978-3-540-75488-6},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/YFN38ABX/Nishida and Yamauchi - 2007 - Detecting Concept Drift Using Statistical Testing.pdf}
}

@article{ormenisHorizontallyScalableML,
  title = {Horizontally {{Scalable ML Pipelines}} with a {{Feature Store}}},
  author = {Ormenis, Alexandru A},
  pages = {2},
  abstract = {Machine Learning (ML) pipelines are the fundamental building block for productionizing ML models. However, much introductory material for machine learning and deep learning emphasizes ad-hoc feature engineering and training pipelines to experiment with ML models. Such pipelines have a tendency to become complex over time and do not allow features to be easily re-used across different pipelines. Duplicating features can even lead to correctness problems when features have different implementations for training and serving. In this demo, we introduce the Feature Store as a new data layer in horizontally scalable machine learning pipelines.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/GN7TQBRA/Ormenis - Horizontally Scalable ML Pipelines with a Feature .pdf;/Users/theofpa/Zotero/storage/KCFPZ8F5/horizontally-scalable-ml-pipelines-with-a-feature-store.html}
}

@unpublished{orrManagingMLPipelines2021,
  title = {Managing {{ML Pipelines}}: {{Feature Stores}} and the {{Coming Wave}} of {{Embedding Ecosystems}}},
  shorttitle = {Managing {{ML Pipelines}}},
  author = {Orr, Laurel and Sanyal, Atindriyo and Ling, Xiao and Goel, Karan and Leszczynski, Megan},
  date = {2021-08-11},
  eprint = {2108.05053},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2108.05053},
  urldate = {2021-10-13},
  abstract = {The industrial machine learning pipeline requires iterating on model features, training and deploying models, and monitoring deployed models at scale. Feature stores were developed to manage and standardize the engineer's workflow in this end-to-end pipeline, focusing on traditional tabular feature data. In recent years, however, model development has shifted towards using self-supervised pretrained embeddings as model features. Managing these embeddings and the downstream systems that use them introduces new challenges with respect to managing embedding training data, measuring embedding quality, and monitoring downstream models that use embeddings. These challenges are largely unaddressed in standard feature stores. Our goal in this tutorial is to introduce the feature store system and discuss the challenges and current solutions to managing these new embedding-centric pipelines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases,Computer Science - Machine Learning},
  file = {/Users/theofpa/Zotero/storage/CKJ9L8HW/Orr et al. - 2021 - Managing ML Pipelines Feature Stores and the Comi.pdf;/Users/theofpa/Zotero/storage/X7VSF2WV/2108.html}
}

@unpublished{paleyesChallengesDeployingMachine2021,
  title = {Challenges in {{Deploying Machine Learning}}: A {{Survey}} of {{Case Studies}}},
  shorttitle = {Challenges in {{Deploying Machine Learning}}},
  author = {Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D.},
  date = {2021-01-18},
  eprint = {2011.09926},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2011.09926},
  urldate = {2021-05-20},
  abstract = {In recent years, machine learning has received increased interest both as an academic research field and as a solution for real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. Our survey shows that practitioners face challenges at each stage of the deployment. The goal of this paper is to layout a research agenda to explore approaches addressing these challenges.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/theofpa/Zotero/storage/XIQY89KW/Paleyes et al. - 2021 - Challenges in Deploying Machine Learning a Survey.pdf}
}

@online{papapanagiotouModelMonitoring2021,
  title = {Model Monitoring},
  author = {Papapanagiotou, Theofilos},
  date = {2021-12-31T15:20:58},
  url = {https://medium.com/prosus-ai-tech-blog/model-monitoring-1849fb3afc1e},
  urldate = {2022-06-25},
  abstract = {In our previous post about the technical capabilities of an ML platform, we’ve highlighted the problem of model degradation over time. In…},
  langid = {english},
  organization = {{Prosus AI Tech Blog}},
  file = {/Users/theofpa/Zotero/storage/2HF8D92R/model-monitoring-1849fb3afc1e.html}
}

@article{pollakOptimalDetectionChange1985,
  title = {Optimal {{Detection}} of a {{Change}} in {{Distribution}}},
  author = {Pollak, Moshe},
  date = {1985-03-01},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  volume = {13},
  number = {1},
  issn = {0090-5364},
  doi = {10.1214/aos/1176346587},
  url = {https://projecteuclid.org/journals/annals-of-statistics/volume-13/issue-1/Optimal-Detection-of-a-Change-in-Distribution/10.1214/aos/1176346587.full},
  urldate = {2022-02-14},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/KAEE5SRR/Pollak - 1985 - Optimal Detection of a Change in Distribution.pdf}
}

@legislation{ProposalRegulationLaying,
  title = {Proposal for a {{Regulation}} Laying down Harmonised Rules on Artificial Intelligence ({{Artificial Intelligence Act}})},
  file = {/Users/theofpa/Zotero/storage/T82D7NU3/Proposal for a Regulation laying down harmonised r.pdf;/Users/theofpa/Zotero/storage/UDYELP5U/cellar%3Ae0649735-a372-11eb-9585-01aa75ed71a1.0001.02%2FDOC_2.pdf;/Users/theofpa/Zotero/storage/USL88E7B/cellar e0649735-a372-11eb-9585-01aa75ed71a1.0001.02_DOC_1.pdf}
}

@inproceedings{rabanserFailingLoudlyEmpirical2019a,
  title = {Failing {{Loudly}}: {{An Empirical Study}} of {{Methods}} for {{Detecting Dataset Shift}}},
  shorttitle = {Failing {{Loudly}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rabanser, Stephan and Günnemann, Stephan and Lipton, Zachary},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/846c260d715e5b854ffad5f70a516c88-Abstract.html},
  urldate = {2021-11-20},
  file = {/Users/theofpa/Zotero/storage/H3C7ZCEK/H3C7ZCEK.pdf}
}

@article{ramdasDecreasingPowerKernel2015,
  title = {On the {{Decreasing Power}} of {{Kernel}} and {{Distance Based Nonparametric Hypothesis Tests}} in {{High Dimensions}}},
  author = {Ramdas, Aaditya and Reddi, Sashank Jakkam and Poczos, Barnabas and Singh, Aarti and Wasserman, Larry},
  date = {2015-03-04},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {29},
  number = {1},
  issn = {2374-3468},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/9692},
  urldate = {2022-01-13},
  abstract = {This paper is about two related decision theoretic problems, nonparametric two-sample testing and independence testing. There is a belief that two recently proposed solutions, based on kernels and distances between pairs of points, behave well in high-dimensional settings. We identify different sources of misconception that give rise to the above belief. Specifically, we differentiate the hardness of estimation of test statistics from the hardness of testing whether these statistics are zero or not, and explicitly discuss a notion of "fair" alternative hypotheses for these problems as dimension increases. We then demonstrate that the power of these tests actually drops polynomially with increasing dimension against fair alternatives. We end with some theoretical insights and shed light on the median heuristic for kernel bandwidth selection. Our work advances the current understanding of the power of modern nonparametric hypothesis tests in high dimensions.},
  issue = {1},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/IE72ZPPF/Ramdas et al. - 2015 - On the Decreasing Power of Kernel and Distance Bas.pdf}
}

@article{ribeiroAccuracyBehavioralTesting2020,
  title = {Beyond {{Accuracy}}: {{Behavioral Testing}} of {{NLP}} Models with {{CheckList}}},
  shorttitle = {Beyond {{Accuracy}}},
  author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  date = {2020-05-08},
  url = {https://arxiv.org/abs/2005.04118v1},
  urldate = {2021-11-03},
  abstract = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/T9SFXDAV/Ribeiro et al. - 2020 - Beyond Accuracy Behavioral Testing of NLP models .pdf;/Users/theofpa/Zotero/storage/MZUP633F/2005.html}
}

@article{sagawaExtendingWILDSBenchmark2021,
  title = {Extending the {{WILDS Benchmark}} for {{Unsupervised Adaptation}}},
  author = {Sagawa, Shiori and Koh, Pang Wei and Lee, Tony and Gao, Irena and Xie, Sang Michael and Shen, Kendrick and Kumar, Ananya and Hu, Weihua and Yasunaga, Michihiro and Marklund, Henrik and Beery, Sara and David, Etienne and Stavness, Ian and Guo, Wei and Leskovec, Jure and Saenko, Kate and Hashimoto, Tatsunori and Levine, Sergey and Finn, Chelsea and Liang, Percy},
  date = {2021-12-09},
  url = {https://arxiv.org/abs/2112.05090v1},
  urldate = {2022-01-26},
  abstract = {Machine learning systems deployed in the wild are often trained on a source distribution but deployed on a different target distribution. Unlabeled data can be a powerful point of leverage for mitigating these distribution shifts, as it is frequently much more available than labeled data. However, existing distribution shift benchmarks for unlabeled data do not reflect the breadth of scenarios that arise in real-world applications. In this work, we present the WILDS 2.0 update, which extends 8 of the 10 datasets in the WILDS benchmark of distribution shifts to include curated unlabeled data that would be realistically obtainable in deployment. To maintain consistency, the labeled training, validation, and test sets, as well as the evaluation metrics, are exactly the same as in the original WILDS benchmark. These datasets span a wide range of applications (from histology to wildlife conservation), tasks (classification, regression, and detection), and modalities (photos, satellite images, microscope slides, text, molecular graphs). We systematically benchmark state-of-the-art methods that leverage unlabeled data, including domain-invariant, self-training, and self-supervised methods, and show that their success on WILDS 2.0 is limited. To facilitate method development and evaluation, we provide an open-source package that automates data loading and contains all of the model architectures and methods used in this paper. Code and leaderboards are available at https://wilds.stanford.edu.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/GBD8UMMV/Sagawa et al. - 2021 - Extending the WILDS Benchmark for Unsupervised Ada.pdf;/Users/theofpa/Zotero/storage/TWGAWNY8/2112.html}
}

@misc{sanhDistilBERTDistilledVersion2020,
  title = {{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter},
  shorttitle = {{{DistilBERT}}, a Distilled Version of {{BERT}}},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  date = {2020-02-29},
  eprint = {1910.01108},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1910.01108},
  urldate = {2022-06-26},
  abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/theofpa/Zotero/storage/VK8A3Y7R/Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf;/Users/theofpa/Zotero/storage/UYA3683J/1910.html}
}

@misc{sanhDistilBERTDistilledVersion2020a,
  title = {{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter},
  shorttitle = {{{DistilBERT}}, a Distilled Version of {{BERT}}},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  date = {2020-02-29},
  eprint = {1910.01108},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.01108},
  url = {http://arxiv.org/abs/1910.01108},
  urldate = {2022-06-27},
  abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/theofpa/Zotero/storage/II2LXPPN/Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf;/Users/theofpa/Zotero/storage/2GG7FGVD/1910.html}
}

@article{schelterAutomatingLargescaleData2018a,
  title = {Automating Large-Scale Data Quality Verification},
  author = {Schelter, Sebastian and Lange, Dustin and Schmidt, Philipp and Celikel, Meltem and Biessmann, Felix and Grafberger, Andreas},
  date = {2018-08},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {11},
  number = {12},
  pages = {1781--1794},
  issn = {2150-8097},
  doi = {10.14778/3229863.3229867},
  url = {https://dl.acm.org/doi/10.14778/3229863.3229867},
  urldate = {2022-06-25},
  abstract = {Modern companies and institutions rely on data to guide every single business process and decision. Missing or incorrect information seriously compromises any decision process downstream. Therefore, a crucial, but tedious task for everyone involved in data processing is to verify the quality of their data. We present a system for automating the verification of data quality at scale, which meets the requirements of production use cases. Our system provides a declarative API, which combines common quality constraints with userdefined validation code, and thereby enables ‘unit tests’ for data. We efficiently execute the resulting constraint validation workload by translating it to aggregation queries on Apache Spark. Our platform supports the incremental validation of data quality on growing datasets, and leverages machine learning, e.g., for enhancing constraint suggestions, for estimating the ‘predictability’ of a column, and for detecting anomalies in historic data quality time series. We discuss our design decisions, describe the resulting system architecture, and present an experimental evaluation on various datasets.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/M6CIK47F/Schelter et al. - 2018 - Automating large-scale data quality verification.pdf}
}

@unpublished{schleglUnsupervisedAnomalyDetection2017,
  title = {Unsupervised {{Anomaly Detection}} with {{Generative Adversarial Networks}} to {{Guide Marker Discovery}}},
  author = {Schlegl, Thomas and Seeböck, Philipp and Waldstein, Sebastian M. and Schmidt-Erfurth, Ursula and Langs, Georg},
  date = {2017-03-17},
  eprint = {1703.05921},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1703.05921},
  urldate = {2021-11-27},
  abstract = {Obtaining models that capture imaging markers relevant for disease progression and treatment monitoring is challenging. Models are typically based on large amounts of data with annotated examples of known markers aiming at automating detection. High annotation effort and the limitation to a vocabulary of known markers limit the power of such approaches. Here, we perform unsupervised learning to identify anomalies in imaging data as candidates for markers. We propose AnoGAN, a deep convolutional generative adversarial network to learn a manifold of normal anatomical variability, accompanying a novel anomaly scoring scheme based on the mapping from image space to a latent space. Applied to new data, the model labels anomalies, and scores image patches indicating their fit into the learned distribution. Results on optical coherence tomography images of the retina demonstrate that the approach correctly identifies anomalous images, such as images containing retinal fluid or hyperreflective foci.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/theofpa/Zotero/storage/8TFECV3V/Schlegl et al. - 2017 - Unsupervised Anomaly Detection with Generative Adv.pdf}
}

@misc{schoelkopfCausalAnticausalLearning2012,
  title = {On {{Causal}} and {{Anticausal Learning}}},
  author = {Schoelkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Sgouritsa, Eleni and Zhang, Kun and Mooij, Joris},
  date = {2012-06-27},
  eprint = {1206.6471},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1206.6471},
  url = {http://arxiv.org/abs/1206.6471},
  urldate = {2022-06-26},
  abstract = {We consider the problem of function estimation in the case where an underlying causal model can be inferred. This has implications for popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. We argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. In particular, we formulate a hypothesis for when semi-supervised learning can help, and corroborate it with empirical results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/theofpa/Zotero/storage/BKF3LAYB/Schoelkopf et al. - 2012 - On Causal and Anticausal Learning.pdf;/Users/theofpa/Zotero/storage/GQXHMW3R/1206.html}
}

@misc{schrouffMaintainingFairnessDistribution2022,
  title = {Maintaining Fairness across Distribution Shift: Do We Have Viable Solutions for Real-World Applications?},
  shorttitle = {Maintaining Fairness across Distribution Shift},
  author = {Schrouff, Jessica and Harris, Natalie and Koyejo, Oluwasanmi and Alabdulmohsin, Ibrahim and Schnider, Eva and Opsahl-Ong, Krista and Brown, Alex and Roy, Subhrajit and Mincu, Diana and Chen, Christina and Dieng, Awa and Liu, Yuan and Natarajan, Vivek and Karthikesalingam, Alan and Heller, Katherine and Chiappa, Silvia and D'Amour, Alexander},
  date = {2022-02-02},
  eprint = {2202.01034},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.01034},
  url = {http://arxiv.org/abs/2202.01034},
  urldate = {2022-06-26},
  abstract = {Fairness and robustness are often considered as orthogonal dimensions when evaluating machine learning models. However, recent work has revealed interactions between fairness and robustness, showing that fairness properties are not necessarily maintained under distribution shift. In healthcare settings, this can result in e.g. a model that performs fairly according to a selected metric in "hospital A" showing unfairness when deployed in "hospital B". While a nascent field has emerged to develop provable fair and robust models, it typically relies on strong assumptions about the shift, limiting its impact for real-world applications. In this work, we explore the settings in which recently proposed mitigation strategies are applicable by referring to a causal framing. Using examples of predictive models in dermatology and electronic health records, we show that real-world applications are complex and often invalidate the assumptions of such methods. Our work hence highlights technical, practical, and engineering gaps that prevent the development of robustly fair machine learning models for real-world applications. Finally, we discuss potential remedies at each step of the machine learning pipeline.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/theofpa/Zotero/storage/JQAFDS4P/Schrouff et al. - 2022 - Maintaining fairness across distribution shift do.pdf;/Users/theofpa/Zotero/storage/FUMEX7JE/2202.html}
}

@online{ScipyStatsKs,
  title = {Scipy.Stats.Ks\_2samp — {{SciPy}} v1.8.1 {{Manual}}},
  url = {https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html},
  urldate = {2022-06-27},
  file = {/Users/theofpa/Zotero/storage/WW4IS8JU/scipy.stats.ks_2samp.html}
}

@article{shafaeiLessBiasedEvaluation2018,
  title = {A {{Less Biased Evaluation}} of {{Out-of-distribution Sample Detectors}}},
  author = {Shafaei, Alireza and Schmidt, Mark and Little, James J.},
  date = {2018-09-13},
  url = {https://arxiv.org/abs/1809.04729v2},
  urldate = {2021-12-05},
  abstract = {In the real world, a learning system could receive an input that is unlike anything it has seen during training. Unfortunately, out-of-distribution samples can lead to unpredictable behaviour. We need to know whether any given input belongs to the population distribution of the training/evaluation data to prevent unpredictable behaviour in deployed systems. A recent surge of interest in this problem has led to the development of sophisticated techniques in the deep learning literature. However, due to the absence of a standard problem definition or an exhaustive evaluation, it is not evident if we can rely on these methods. What makes this problem different from a typical supervised learning setting is that the distribution of outliers used in training may not be the same as the distribution of outliers encountered in the application. Classical approaches that learn inliers vs. outliers with only two datasets can yield optimistic results. We introduce OD-test, a three-dataset evaluation scheme as a more reliable strategy to assess progress on this problem. We present an exhaustive evaluation of a broad set of methods from related areas on image classification tasks. Contrary to the existing results, we show that for realistic applications of high-dimensional images the previous techniques have low accuracy and are not reliable in practice.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/HYIUFP5J/Shafaei et al. - 2018 - A Less Biased Evaluation of Out-of-distribution Sa.pdf;/Users/theofpa/Zotero/storage/GBIJHIQU/1809.html}
}

@article{stewartMeasuringPredictingVisualizing,
  title = {Measuring, {{Predicting}} and {{Visualizing Short-Term Change}} in {{Word Representation}} and {{Usage}} in {{VKontakte Social Network}}},
  author = {Stewart, Ian and Arendt, Dustin and Bell, Eric and Volkova, Svitlana},
  pages = {12},
  abstract = {Language in social media is extremely dynamic: new words emerge, trend and disappear, while the meaning of existing words can fluctuate over time. Such dynamics are especially notable during a period of crisis. This work addresses several important tasks of measuring, visualizing and predicting short term text representation shift, i.e. the change in a word’s contextual semantics, and contrasting such shift with surface level word dynamics, or concept drift, observed in social media streams. Unlike previous approaches on learning text representations in text, we study the relationship between short-term concept drift and representation shift on a large social media corpus – VKontakte posts in Russian collected during the Russia-Ukraine crisis in 2014 – 2015. Our novel contributions include quantitative and qualitative approaches to (1) measure short-term representation shift and contrast it with surface level concept drift; (2) build predictive models to forecast short-term shifts in meaning from previous meaning as well as from concept drift; and (3) visualize short-term representation shift for example keywords to demonstrate the practical use of our approach to discover and track meaning of newly emerging terms in social media. We show that shortterm representation shift can be accurately predicted up to several weeks in advance. Our unique approach to modeling and visualizing word representation shifts in social media can be used to explore and characterize specific aspects of the streaming corpus during crisis events and potentially improve other downstream classification tasks including realtime event detection.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/F56FFXIJ/Stewart et al. - Measuring, Predicting and Visualizing Short-Term C.pdf}
}

@article{stewartMeasuringPredictingVisualizinga,
  title = {Measuring, {{Predicting}} and {{Visualizing Short-Term Change}} in {{Word Representation}} and {{Usage}} in {{VKontakte Social Network}}},
  author = {Stewart, Ian and Arendt, Dustin and Bell, Eric and Volkova, Svitlana},
  pages = {12},
  abstract = {Language in social media is extremely dynamic: new words emerge, trend and disappear, while the meaning of existing words can fluctuate over time. Such dynamics are especially notable during a period of crisis. This work addresses several important tasks of measuring, visualizing and predicting short term text representation shift, i.e. the change in a word’s contextual semantics, and contrasting such shift with surface level word dynamics, or concept drift, observed in social media streams. Unlike previous approaches on learning text representations in text, we study the relationship between short-term concept drift and representation shift on a large social media corpus – VKontakte posts in Russian collected during the Russia-Ukraine crisis in 2014 – 2015. Our novel contributions include quantitative and qualitative approaches to (1) measure short-term representation shift and contrast it with surface level concept drift; (2) build predictive models to forecast short-term shifts in meaning from previous meaning as well as from concept drift; and (3) visualize short-term representation shift for example keywords to demonstrate the practical use of our approach to discover and track meaning of newly emerging terms in social media. We show that shortterm representation shift can be accurately predicted up to several weeks in advance. Our unique approach to modeling and visualizing word representation shifts in social media can be used to explore and characterize specific aspects of the streaming corpus during crisis events and potentially improve other downstream classification tasks including realtime event detection.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/5JBC6T2J/Stewart et al. - Measuring, Predicting and Visualizing Short-Term C.pdf}
}

@unpublished{truongSelectiveReviewOffline2020,
  title = {Selective Review of Offline Change Point Detection Methods},
  author = {Truong, Charles and Oudre, Laurent and Vayatis, Nicolas},
  date = {2020-03-26},
  eprint = {1801.00718},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.1016/j.sigpro.2019.107299},
  url = {http://arxiv.org/abs/1801.00718},
  urldate = {2021-11-27},
  abstract = {This article presents a selective survey of algorithms for the offline detection of multiple change points in multivariate time series. A general yet structuring methodological strategy is adopted to organize this vast body of work. More precisely, detection algorithms considered in this review are characterized by three elements: a cost function, a search method and a constraint on the number of changes. Each of those elements is described, reviewed and discussed separately. Implementations of the main algorithms described in this article are provided within a Python package called ruptures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computational Engineering; Finance; and Science,Statistics - Computation,Statistics - Methodology},
  file = {/Users/theofpa/Zotero/storage/8R847QW4/Truong et al. - 2020 - Selective review of offline change point detection.pdf}
}

@article{tsymbalProblemConceptDrift,
  title = {The Problem of Concept Drift: Definitions and Related Work},
  author = {Tsymbal, Alexey},
  pages = {7},
  abstract = {In the real world concepts are often not stable but change with time. Typical examples of this are weather prediction rules and customers’ preferences. The underlying data distribution may change as well. Often these changes make the model built on old data inconsistent with the new data, and regular updating of the model is necessary. This problem, known as concept drift, complicates the task of learning a model from data and requires special approaches, different from commonly used techniques, which treat arriving instances as equally important contributors to the final concept. This paper considers different types of concept drift, peculiarities of the problem, and gives a critical review of existing approaches to the problem.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/KC2JWWPD/Tsymbal - The problem of concept drift definitions and rela.pdf}
}

@software{vanlooverenAlibiDetectAlgorithms2022,
  title = {Alibi {{Detect}}: {{Algorithms}} for Outlier, Adversarial and Drift Detection},
  shorttitle = {Alibi {{Detect}}},
  author = {Van Looveren, Arnaud and Klaise, Janis and Vacanti, Giovanni and Cobb, Oliver and Scillitoe, Ashley and Samoilescu, Robert},
  date = {2022-04},
  origdate = {2019-10-07T13:29:13Z},
  url = {https://github.com/SeldonIO/alibi-detect},
  urldate = {2022-06-26},
  abstract = {Algorithms for outlier, adversarial and drift detection},
  version = {0.9.1}
}

@inproceedings{wangMilvusPurposeBuiltVector2021,
  title = {Milvus: {{A Purpose-Built Vector Data Management System}}},
  shorttitle = {Milvus},
  booktitle = {Proceedings of the 2021 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Wang, Jianguo and Yi, Xiaomeng and Guo, Rentong and Jin, Hai and Xu, Peng and Li, Shengjun and Wang, Xiangyu and Guo, Xiangzhou and Li, Chengming and Xu, Xiaohai and Yu, Kun and Yuan, Yuxing and Zou, Yinghao and Long, Jiquan and Cai, Yudong and Li, Zhenxiang and Zhang, Zhifeng and Mo, Yihua and Gu, Jun and Jiang, Ruiyi and Wei, Yi and Xie, Charles},
  date = {2021-06-09},
  series = {{{SIGMOD}}/{{PODS}} '21},
  pages = {2614--2627},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3448016.3457550},
  url = {https://doi.org/10.1145/3448016.3457550},
  urldate = {2021-11-08},
  abstract = {Recently, there has been a pressing need to manage high-dimensional vector data in data science and AI applications. This trend is fueled by the proliferation of unstructured data and machine learning (ML), where ML models usually transform unstructured data into feature vectors for data analytics, e.g., product recommendation. Existing systems and algorithms for managing vector data have two limitations: (1) They incur serious performance issue when handling large-scale and dynamic vector data; and (2) They provide limited functionalities that cannot meet the requirements of versatile applications. This paper presents Milvus, a purpose-built data management system to efficiently manage large-scale vector data. Milvus supports easy-to-use application interfaces (including SDKs and RESTful APIs); optimizes for the heterogeneous computing platform with modern CPUs and GPUs; enables advanced query processing beyond simple vector similarity search; handles dynamic data for fast updates while ensuring efficient query processing; and distributes data across multiple nodes to achieve scalability and availability. We first describe the design and implementation of Milvus. Then we demonstrate the real-world use cases supported by Milvus. In particular, we build a series of 10 applications (e.g., image/video search, chemical structure analysis, COVID-19 dataset search, personalized recommendation, biological multi-factor authentication, intelligent question answering) on top of Milvus. Finally, we experimentally evaluate Milvus with a wide range of systems including two open source systems (Vearch and Microsoft SPTAG) and three commercial systems. Experiments show that Milvus is up to two orders of magnitude faster than the competitors while providing more functionalities. Now Milvus is deployed by hundreds of organizations worldwide and it is also recognized as an incubation-stage project of the LF AI \& Data Foundation. Milvus is open-sourced at https://github.com/milvus-io/milvus.},
  isbn = {978-1-4503-8343-1},
  keywords = {data science,heterogeneous computing,high-dimensional similarity search,machine learning,vector database},
  file = {/Users/theofpa/Zotero/storage/VIPV8NWA/Wang et al. - 2021 - Milvus A Purpose-Built Vector Data Management Sys.pdf}
}

@article{weiAnalyticDBVHybridAnalytical2020,
  title = {{{AnalyticDB-V}}: A Hybrid Analytical Engine towards Query Fusion for Structured and Unstructured Data},
  shorttitle = {{{AnalyticDB-V}}},
  author = {Wei, Chuangxian and Wu, Bin and Wang, Sheng and Lou, Renjie and Zhan, Chaoqun and Li, Feifei and Cai, Yuanzhe},
  date = {2020-08-01},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {13},
  number = {12},
  pages = {3152--3165},
  issn = {2150-8097},
  doi = {10.14778/3415478.3415541},
  url = {https://doi.org/10.14778/3415478.3415541},
  urldate = {2021-11-13},
  abstract = {With the explosive growth of unstructured data (such as images, videos, and audios), unstructured data analytics is widespread in a rich vein of real-world applications. Many database systems start to incorporate unstructured data analysis to meet such demands. However, queries over unstructured and structured data are often treated as disjoint tasks in most systems, where hybrid queries (i.e., involving both data types) are not yet fully supported. In this paper, we present a hybrid analytic engine developed at Alibaba, named AnalyticDB-V (ADBV), to fulfill such emerging demands. ADBV offers an interface that enables users to express hybrid queries using SQL semantics by converting unstructured data to high dimensional vectors. ADBV adopts the lambda framework and leverages the merits of approximate nearest neighbor search (ANNS) techniques to support hybrid data analytics. Moreover, a novel ANNS algorithm is proposed to improve the accuracy on large-scale vectors representing massive unstructured data. All ANNS algorithms are implemented as physical operators in ADBV, meanwhile, accuracy-aware cost-based optimization techniques are proposed to identify effective execution plans. Experimental results on both public and in-house datasets show the superior performance achieved by ADBV and its effectiveness. ADBV has been successfully deployed on Alibaba Cloud to provide hybrid query processing services for various real-world applications.},
  file = {/Users/theofpa/Zotero/storage/SJWEU7YD/Wei et al. - 2020 - AnalyticDB-V a hybrid analytical engine towards q.pdf}
}

@inproceedings{zhaoComparingDistributionsMeasuring2022,
  title = {Comparing {{Distributions}} by {{Measuring Differences}} That {{Affect Decision Making}}},
  author = {Zhao, Shengjia and Sinha, Abhishek and He, Yutong and Perreault, Aidan and Song, Jiaming and Ermon, Stefano},
  date = {2022-03-17},
  url = {https://openreview.net/forum?id=KB5onONJIAU},
  urldate = {2022-06-26},
  abstract = {Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal loss for a...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/9SDUN7UD/Zhao et al. - 2022 - Comparing Distributions by Measuring Differences t.pdf;/Users/theofpa/Zotero/storage/JL9MUN6D/forum.html}
}

@incollection{zliobaiteOverviewConceptDrift2016,
  title = {An {{Overview}} of {{Concept Drift Applications}}},
  booktitle = {Big {{Data Analysis}}: {{New Algorithms}} for a {{New Society}}},
  author = {Žliobaitė, Indrė and Pechenizkiy, Mykola and Gama, João},
  editor = {Japkowicz, Nathalie and Stefanowski, Jerzy},
  date = {2016},
  series = {Studies in {{Big Data}}},
  volume = {16},
  pages = {91--114},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-26989-4_4},
  url = {http://link.springer.com/10.1007/978-3-319-26989-4_4},
  urldate = {2022-06-26},
  abstract = {In most challenging data analysis applications, data evolve over time and must be analyzed in near real time. Patterns and relations in such data often evolve over time, thus, models built for analyzing such data quickly become obsolete over time. In machine learning and data mining this phenomenon is referred to as concept drift. The objective is to deploy models that would diagnose themselves and adapt to changing data over time. This chapter provides an application oriented view towards concept drift research, with a focus on supervised learning tasks. First we overview and categorize application tasks for which the problem of concept drift is particularly relevant. Then we construct a reference framework for positioning application tasks within a spectrum of problems related to concept drift. Finally, we discuss some promising research directions from the application perspective, and present recommendations for application driven concept drift research and development.},
  isbn = {978-3-319-26987-0 978-3-319-26989-4},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/L9Q2KEAV/Žliobaitė et al. - 2016 - An Overview of Concept Drift Applications.pdf}
}


