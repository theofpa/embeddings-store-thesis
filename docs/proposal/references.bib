
@online{azarbonyadWordsAreMalleable2017,
  title = {Words Are {{Malleable}}: Computing {{Semantic Shifts}} in {{Political}} and {{Media Discourse}}},
  shorttitle = {Words Are {{Malleable}}},
  author = {Azarbonyad, Hosein and Dehghani, Mostafa and Beelen, Kaspar and Arkut, Alexandra and Marx, Maarten and Kamps, Jaap},
  date = {2017-11-15},
  eprint = {1711.05603},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1711.05603},
  urldate = {2021-10-29},
  abstract = {Recently, researchers started to pay attention to the detection of temporal shifts in the meaning of words. However, most (if not all) of these approaches restricted their efforts to uncovering change over time, thus neglecting other valuable dimensions such as social or political variability. We propose an approach for detecting semantic shifts between different viewpoints—broadly defined as a set of texts that share a specific metadata feature, which can be a time-period, but also a social entity such as a political party. For each viewpoint, we learn a semantic space in which each word is represented as a low dimensional neural embedded vector. The challenge is to compare the meaning of a word in one space to its meaning in another space and measure the size of the semantic shifts. We compare the effectiveness of a measure based on optimal transformations between the two spaces with a measure based on the similarity of the neighbors of the word in the respective spaces. Our experiments demonstrate that the combination of these two performs best. We show that the semantic shifts not only occur over time, but also along different viewpoints in a short period of time. For evaluation, we demonstrate how this approach captures meaningful semantic shifts and can help improve other tasks such as the contrastive viewpoint summarization and ideology detection (measured as classification accuracy) in political texts. We also show that the two laws of semantic change which were empirically shown to hold for temporal shifts also hold for shifts across viewpoints. These laws state that frequent words are less likely to shift meaning while words with many senses are more likely to do so.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/theofpa/Zotero/storage/HZB3YFQT/Azarbonyad et al. - 2017 - Words are Malleable Computing Semantic Shifts in .pdf}
}

@inproceedings{baylorContinuousTrainingProduction2019a,
  title = {Continuous {{Training}} for {{Production}} \{\vphantom\}{{ML}}\vphantom\{\} in the {{TensorFlow Extended}} (\{\vphantom\}{{TFX}}\vphantom\{\}) {{Platform}}},
  author = {Baylor, Denis and Haas, Kevin and Katsiapis, Konstantinos and Leong, Sammy and Liu, Rose and Menwald, Clemens and Miao, Hui and Polyzotis, Neoklis and Trott, Mitchell and Zinkevich, Martin},
  date = {2019},
  pages = {51--53},
  url = {https://www.usenix.org/conference/opml19/presentation/baylor},
  urldate = {2021-11-03},
  eventtitle = {2019 \{\vphantom\}{{USENIX}}\vphantom\{\} {{Conference}} on {{Operational Machine Learning}} ({{OpML}} 19)},
  isbn = {978-1-939133-00-7},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/ZHEQUKVE/Baylor et al. - 2019 - Continuous Training for Production ML in the Ten.pdf;/Users/theofpa/Zotero/storage/29J2LFUL/baylor.html}
}

@inproceedings{benderClimbingNLUMeaning2020,
  title = {Climbing towards {{NLU}}: On {{Meaning}}, {{Form}}, and {{Understanding}} in the {{Age}} of {{Data}}},
  shorttitle = {Climbing towards {{NLU}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Bender, Emily M. and Koller, Alexander},
  date = {2020-07},
  pages = {5185--5198},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.463},
  url = {https://aclanthology.org/2020.acl-main.463},
  urldate = {2021-11-03},
  abstract = {The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We've Been and Where We're Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.},
  eventtitle = {{{ACL}} 2020},
  file = {/Users/theofpa/Zotero/storage/65JTB9XV/Bender and Koller - 2020 - Climbing towards NLU On Meaning, Form, and Unders.pdf}
}

@article{breckDataValidationMachine,
  title = {Data {{Validation}} for {{Machine Learning}}},
  author = {Breck, Eric and Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong and Zinkevich, Martin},
  pages = {14},
  abstract = {Machine learning is a powerful tool for gleaning knowledge from massive amounts of data. While a great deal of machine learning research has focused on improving the accuracy and efficiency of training and inference algorithms, there is less attention in the equally important problem of monitoring the quality of data fed to machine learning. The importance of this problem is hard to dispute: errors in the input data can nullify any benefits on speed and accuracy for training and inference. This argument points to a data-centric approach to machine learning that treats training and serving data as an important production asset, on par with the algorithm and infrastructure used for learning.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/98EDTBUL/Breck et al. - Data Validation for Machine Learning.pdf}
}

@inproceedings{cavenessTensorFlowDataValidation2020,
  title = {{{TensorFlow Data Validation}}: Data {{Analysis}} and {{Validation}} in {{Continuous ML Pipelines}}},
  shorttitle = {{{TensorFlow Data Validation}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Caveness, Emily and G. C., Paul Suganthan and Peng, Zhuo and Polyzotis, Neoklis and Roy, Sudip and Zinkevich, Martin},
  date = {2020-06-11},
  series = {{{SIGMOD}} '20},
  pages = {2793--2796},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3318464.3384707},
  url = {https://doi.org/10.1145/3318464.3384707},
  urldate = {2021-11-03},
  abstract = {Machine Learning (ML) research has primarily focused on improving the accuracy and efficiency of the training algorithms while paying much less attention to the equally important problem of understanding, validating, and monitoring the data fed to ML. Irrespective of the ML algorithms used, data errors can adversely affect the quality of the generated model. This indicates that we need to adopt a data-centric approach to ML that treats data as a first-class citizen, on par with algorithms and infrastructure which are the typical building blocks of ML pipelines. In this demonstration we showcase TensorFlow Data Validation (TFDV), a scalable data analysis and validation system for ML that we have developed at Google and recently open-sourced. This system is deployed in production as an integral part of TFX - an end-to-end machine learning platform at Google. It is used by hundreds of product teams at Google and has received significant attention from the open-source community as well.},
  isbn = {978-1-4503-6735-6},
  keywords = {data management,machine learning},
  file = {/Users/theofpa/Zotero/storage/7KPM5IES/Caveness et al. - 2020 - TensorFlow Data Validation Data Analysis and Vali.pdf}
}

@article{deltrediciShortTermMeaningShift2018,
  title = {Short-{{Term Meaning Shift}}: A {{Distributional Exploration}}},
  shorttitle = {Short-{{Term Meaning Shift}}},
  author = {Del Tredici, Marco and Fernández, Raquel and Boleda, Gemma},
  date = {2018-09-10},
  url = {https://arxiv.org/abs/1809.03169v3},
  urldate = {2021-10-29},
  abstract = {We present the first exploration of meaning shift over short periods of time in online communities using distributional representations. We create a small annotated dataset and use it to assess the performance of a standard model for meaning shift detection on short-term meaning shift. We find that the model has problems distinguishing meaning shift from referential phenomena, and propose a measure of contextual variability to remedy this.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/8JTYSC72/Del Tredici et al. - 2018 - Short-Term Meaning Shift A Distributional Explora.pdf;/Users/theofpa/Zotero/storage/8IS3J2P3/1809.html}
}

@article{goelRobustnessGymUnifying2021,
  title = {Robustness {{Gym}}: Unifying the {{NLP Evaluation Landscape}}},
  shorttitle = {Robustness {{Gym}}},
  author = {Goel, Karan and Rajani, Nazneen and Vig, Jesse and Tan, Samson and Wu, Jason and Zheng, Stephan and Xiong, Caiming and Bansal, Mohit and Ré, Christopher},
  date = {2021-01-13},
  url = {https://arxiv.org/abs/2101.04840v1},
  urldate = {2021-11-03},
  abstract = {Despite impressive performance on standard benchmarks, deep neural networks are often brittle when deployed in real-world systems. Consequently, recent research has focused on testing the robustness of such models, resulting in a diverse set of evaluation methodologies ranging from adversarial attacks to rule-based data transformations. In this work, we identify challenges with evaluating NLP systems and propose a solution in the form of Robustness Gym (RG), a simple and extensible evaluation toolkit that unifies 4 standard evaluation paradigms: subpopulations, transformations, evaluation sets, and adversarial attacks. By providing a common platform for evaluation, Robustness Gym enables practitioners to compare results from all 4 evaluation paradigms with just a few clicks, and to easily develop and share novel evaluation methods using a built-in set of abstractions. To validate Robustness Gym's utility to practitioners, we conducted a real-world case study with a sentiment-modeling team, revealing performance degradations of 18\%+. To verify that Robustness Gym can aid novel research analyses, we perform the first study of state-of-the-art commercial and academic named entity linking (NEL) systems, as well as a fine-grained analysis of state-of-the-art summarization models. For NEL, commercial systems struggle to link rare entities and lag their academic counterparts by 10\%+, while state-of-the-art summarization models struggle on examples that require abstraction and distillation, degrading by 9\%+. Robustness Gym can be found at https://robustnessgym.com/},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/JZXACBJZ/Goel et al. - 2021 - Robustness Gym Unifying the NLP Evaluation Landsc.pdf;/Users/theofpa/Zotero/storage/Y27AITIR/2101.html}
}

@article{hamiltonDiachronicWordEmbeddings2016,
  title = {Diachronic {{Word Embeddings Reveal Statistical Laws}} of {{Semantic Change}}},
  author = {Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
  date = {2016-05-30},
  url = {https://arxiv.org/abs/1605.09096v6},
  urldate = {2021-11-03},
  abstract = {Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity---the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation---independent of frequency, words that are more polysemous have higher rates of semantic change.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/IS9MAAPN/Hamilton et al. - 2016 - Diachronic Word Embeddings Reveal Statistical Laws.pdf;/Users/theofpa/Zotero/storage/XD9YKR3M/1605.html}
}

@article{kimTemporalAnalysisLanguage2014,
  title = {Temporal {{Analysis}} of {{Language}} through {{Neural Language Models}}},
  author = {Kim, Yoon and Chiu, Yi-I. and Hanaki, Kentaro and Hegde, Darshan and Petrov, Slav},
  date = {2014-05-14},
  url = {https://arxiv.org/abs/1405.3515v1},
  urldate = {2021-11-03},
  abstract = {We provide a method for automatically detecting change in language across time through a chronologically trained neural language model. We train the model on the Google Books Ngram corpus to obtain word vector representations specific to each year, and identify words that have changed significantly from 1900 to 2009. The model identifies words such as "cell" and "gay" as having changed during that time period. The model simultaneously identifies the specific years during which such words underwent change.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/696CXERW/Kim et al. - 2014 - Temporal Analysis of Language through Neural Langu.pdf;/Users/theofpa/Zotero/storage/X6NJQDIM/1405.html}
}

@inproceedings{kulkarniStatisticallySignificantDetection2015,
  title = {Statistically {{Significant Detection}} of {{Linguistic Change}}},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{World Wide Web}}},
  author = {Kulkarni, Vivek and Al-Rfou, Rami and Perozzi, Bryan and Skiena, Steven},
  date = {2015-05-18},
  series = {{{WWW}} '15},
  pages = {625--635},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  location = {{Republic and Canton of Geneva, CHE}},
  doi = {10.1145/2736277.2741627},
  url = {https://doi.org/10.1145/2736277.2741627},
  urldate = {2021-11-03},
  abstract = {We propose a new computational approach for tracking and detecting statistically significant linguistic shifts in the meaning and usage of words. Such linguistic shifts are especially prevalent on the Internet, where the rapid exchange of ideas can quickly change a word's meaning. Our meta-analysis approach constructs property time series of word usage, and then uses statistically sound change point detection algorithms to identify significant linguistic shifts. We consider and analyze three approaches of increasing complexity to generate such linguistic property time series, the culmination of which uses distributional characteristics inferred from word co-occurrences. Using recently proposed deep neural language models, we first train vector representations of words for each time period. Second, we warp the vector spaces into one unified coordinate system. Finally, we construct a distance-based distributional time series for each word to track its linguistic displacement over time. We demonstrate that our approach is scalable by tracking linguistic change across years of micro-blogging using Twitter, a decade of product reviews using a corpus of movie reviews from Amazon, and a century of written books using the Google Book Ngrams. Our analysis reveals interesting patterns of language usage change commensurate with each medium.},
  isbn = {978-1-4503-3469-3},
  keywords = {computational linguistics,web mining},
  file = {/Users/theofpa/Zotero/storage/492IHCD9/Kulkarni et al. - 2015 - Statistically Significant Detection of Linguistic .pdf}
}

@inproceedings{mikolovDistributedRepresentationsWords2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  date = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
  urldate = {2021-10-29},
  file = {/Users/theofpa/Zotero/storage/K7F35TT3/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf}
}

@article{millerEffectNaturalDistribution2020,
  title = {The {{Effect}} of {{Natural Distribution Shift}} on {{Question Answering Models}}},
  author = {Miller, John and Krauth, Karl and Recht, Benjamin and Schmidt, Ludwig},
  date = {2020-04-29},
  url = {https://arxiv.org/abs/2004.14444v1},
  urldate = {2021-11-03},
  abstract = {We build four new test sets for the Stanford Question Answering Dataset (SQuAD) and evaluate the ability of question-answering systems to generalize to new data. Our first test set is from the original Wikipedia domain and measures the extent to which existing systems overfit the original test set. Despite several years of heavy test set re-use, we find no evidence of adaptive overfitting. The remaining three test sets are constructed from New York Times articles, Reddit posts, and Amazon product reviews and measure robustness to natural distribution shifts. Across a broad range of models, we observe average performance drops of 3.8, 14.0, and 17.4 F1 points, respectively. In contrast, a strong human baseline matches or exceeds the performance of SQuAD models on the original domain and exhibits little to no drop in new domains. Taken together, our results confirm the surprising resilience of the holdout method and emphasize the need to move towards evaluation metrics that incorporate robustness to natural distribution shifts.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/QZAK3NWV/Miller et al. - 2020 - The Effect of Natural Distribution Shift on Questi.pdf;/Users/theofpa/Zotero/storage/2SXGYHNZ/2004.html}
}

@online{ModelDBProceedingsWorkshop,
  title = {{{ModelDB}} | {{Proceedings}} of the {{Workshop}} on {{Human}}-{{In}}-the-{{Loop Data Analytics}}},
  url = {https://dl.acm.org/doi/10.1145/2939502.2939516},
  urldate = {2021-11-03},
  file = {/Users/theofpa/Zotero/storage/MSEXWQ2A/Vartak et al. - 2016 - M odelspa.pdf;/Users/theofpa/Zotero/storage/YEL2PX3L/2939502.html}
}

@article{morrisTextAttackFrameworkAdversarial2020,
  title = {{{TextAttack}}: A {{Framework}} for {{Adversarial Attacks}}, {{Data Augmentation}}, and {{Adversarial Training}} in {{NLP}}},
  shorttitle = {{{TextAttack}}},
  author = {Morris, John X. and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
  date = {2020-04-29},
  url = {https://arxiv.org/abs/2005.05909v4},
  urldate = {2021-11-03},
  abstract = {While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack's modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness. TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/BBVH3T9W/Morris et al. - 2020 - TextAttack A Framework for Adversarial Attacks, D.pdf;/Users/theofpa/Zotero/storage/VEGKLKTB/2005.html}
}

@article{ormenisHorizontallyScalableML,
  title = {Horizontally {{Scalable ML Pipelines}} with a {{Feature Store}}},
  author = {Ormenis, Alexandru A},
  pages = {2},
  abstract = {Machine Learning (ML) pipelines are the fundamental building block for productionizing ML models. However, much introductory material for machine learning and deep learning emphasizes ad-hoc feature engineering and training pipelines to experiment with ML models. Such pipelines have a tendency to become complex over time and do not allow features to be easily re-used across different pipelines. Duplicating features can even lead to correctness problems when features have different implementations for training and serving. In this demo, we introduce the Feature Store as a new data layer in horizontally scalable machine learning pipelines.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/GN7TQBRA/Ormenis - Horizontally Scalable ML Pipelines with a Feature .pdf;/Users/theofpa/Zotero/storage/KCFPZ8F5/horizontally-scalable-ml-pipelines-with-a-feature-store.html}
}

@online{orrManagingMLPipelines2021a,
  title = {Managing {{ML Pipelines}}: Feature {{Stores}} and the {{Coming Wave}} of {{Embedding Ecosystems}}},
  shorttitle = {Managing {{ML Pipelines}}},
  author = {Orr, Laurel and Sanyal, Atindriyo and Ling, Xiao and Goel, Karan and Leszczynski, Megan},
  date = {2021-08-11},
  eprint = {2108.05053},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2108.05053},
  urldate = {2021-10-13},
  abstract = {The industrial machine learning pipeline requires iterating on model features, training and deploying models, and monitoring deployed models at scale. Feature stores were developed to manage and standardize the engineer's workflow in this end-to-end pipeline, focusing on traditional tabular feature data. In recent years, however, model development has shifted towards using self-supervised pretrained embeddings as model features. Managing these embeddings and the downstream systems that use them introduces new challenges with respect to managing embedding training data, measuring embedding quality, and monitoring downstream models that use embeddings. These challenges are largely unaddressed in standard feature stores. Our goal in this tutorial is to introduce the feature store system and discuss the challenges and current solutions to managing these new embedding-centric pipelines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases,Computer Science - Machine Learning},
  file = {/Users/theofpa/Zotero/storage/CKJ9L8HW/Orr et al. - 2021 - Managing ML Pipelines Feature Stores and the Comi.pdf;/Users/theofpa/Zotero/storage/X7VSF2WV/2108.html}
}

@article{ribeiroAccuracyBehavioralTesting2020,
  title = {Beyond {{Accuracy}}: Behavioral {{Testing}} of {{NLP}} Models with {{CheckList}}},
  shorttitle = {Beyond {{Accuracy}}},
  author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  date = {2020-05-08},
  url = {https://arxiv.org/abs/2005.04118v1},
  urldate = {2021-11-03},
  abstract = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/T9SFXDAV/Ribeiro et al. - 2020 - Beyond Accuracy Behavioral Testing of NLP models .pdf;/Users/theofpa/Zotero/storage/MZUP633F/2005.html}
}

@article{stewartMeasuringPredictingVisualizing,
  title = {Measuring, {{Predicting}} and {{Visualizing Short}}-{{Term Change}} in {{Word Representation}} and {{Usage}} in {{VKontakte Social Network}}},
  author = {Stewart, Ian and Arendt, Dustin and Bell, Eric and Volkova, Svitlana},
  pages = {12},
  abstract = {Language in social media is extremely dynamic: new words emerge, trend and disappear, while the meaning of existing words can fluctuate over time. Such dynamics are especially notable during a period of crisis. This work addresses several important tasks of measuring, visualizing and predicting short term text representation shift, i.e. the change in a word’s contextual semantics, and contrasting such shift with surface level word dynamics, or concept drift, observed in social media streams. Unlike previous approaches on learning text representations in text, we study the relationship between short-term concept drift and representation shift on a large social media corpus – VKontakte posts in Russian collected during the Russia-Ukraine crisis in 2014 – 2015. Our novel contributions include quantitative and qualitative approaches to (1) measure short-term representation shift and contrast it with surface level concept drift; (2) build predictive models to forecast short-term shifts in meaning from previous meaning as well as from concept drift; and (3) visualize short-term representation shift for example keywords to demonstrate the practical use of our approach to discover and track meaning of newly emerging terms in social media. We show that shortterm representation shift can be accurately predicted up to several weeks in advance. Our unique approach to modeling and visualizing word representation shifts in social media can be used to explore and characterize specific aspects of the streaming corpus during crisis events and potentially improve other downstream classification tasks including realtime event detection.},
  langid = {english},
  file = {/Users/theofpa/Zotero/storage/F56FFXIJ/Stewart et al. - Measuring, Predicting and Visualizing Short-Term C.pdf}
}


